{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# Load the trained BPE tokenizer\n",
    "bpe_tokenizer = spm.SentencePieceProcessor()\n",
    "bpe_tokenizer.load('../tokenizer/spbpe_isc.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import morfessor\n",
    "\n",
    "io = morfessor.MorfessorIO()\n",
    "morfessor_model = morfessor.BaselineModel()\n",
    "\n",
    "# load Morfessor model \n",
    "morfessor_model = io.read_binary_model_file('../morfessor/morf_isc_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_bpe(sentence, tokenizer):\n",
    "    # SentencePiece BPE segmentation\n",
    "    return tokenizer.encode(sentence, out_type=str)\n",
    "\n",
    "def segment_morfessor(sentence, model):\n",
    "    # Morfessor segmentation\n",
    "    words = sentence.split()\n",
    "    segmented_words = []\n",
    "    for word in words:\n",
    "        segments = model.viterbi_segment(word)[0]\n",
    "        segmented_words.extend(segments)\n",
    "    return segmented_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "\n",
    "with open('../data/isc_sentences.txt', 'r') as f:\n",
    "    sentences = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_bpe = [segment_bpe(sentence, bpe_tokenizer) for sentence in sentences]\n",
    "sentences_morfessor = [segment_morfessor(sentence, morfessor_model) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Step 1: Split tokenized_bpe into train and test sets\n",
    "train_data, test_data = train_test_split(sentences_bpe, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Initialize mBERT tokenizer\n",
    "mbert_tokenizerBPE = BertTokenizer.from_pretrained(\"bert-base-multilingual-uncased\")\n",
    "\n",
    "# Tokenize segmented sentences for mBERT\n",
    "def prepare_tokenized_inputs(sentences_segmented):\n",
    "    tokenized_data = []\n",
    "    for sentence in sentences_segmented:\n",
    "        # Join segments with space (as if reconstructing sentence with segmented tokens)\n",
    "        segmented_text = \" \".join(sentence)\n",
    "        tokenized_data.append(mbert_tokenizerBPE(segmented_text, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\"))\n",
    "    return tokenized_data\n",
    "\n",
    "# Apply tokenization\n",
    "tokenized_bpe = prepare_tokenized_inputs(train_data)\n",
    "# tokenized_morfessor = prepare_tokenized_inputs(sentences_morfessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/atrujillo/langmodels/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='1923' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   5/1923 00:00 < 08:36, 3.72 it/s, Epoch 0.01/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 41\u001b[0m\n\u001b[1;32m     33\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     34\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodelBPE,\n\u001b[1;32m     35\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m     36\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mdataset,\n\u001b[1;32m     37\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_collator,\n\u001b[1;32m     38\u001b[0m )\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Fine-tune the model (checkpoints will automatically be saved at specified steps)\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Save the final model and tokenizer\u001b[39;00m\n\u001b[1;32m     44\u001b[0m modelBPE\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./mbert_BPE\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/langmodels/lib/python3.12/site-packages/transformers/trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/langmodels/lib/python3.12/site-packages/transformers/trainer.py:2481\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2475\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2476\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2477\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2478\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2479\u001b[0m )\n\u001b[1;32m   2480\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2481\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2484\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2485\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2486\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2487\u001b[0m ):\n\u001b[1;32m   2488\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2489\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/langmodels/lib/python3.12/site-packages/transformers/trainer.py:3579\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3576\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3578\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3579\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3581\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3583\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3584\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3585\u001b[0m ):\n",
      "File \u001b[0;32m~/langmodels/lib/python3.12/site-packages/transformers/trainer.py:3633\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3631\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[1;32m   3632\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[0;32m-> 3633\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3634\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3635\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/langmodels/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/langmodels/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/langmodels/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py:193\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodule_kwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    192\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[: \u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 193\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_device)\n",
      "File \u001b[0;32m~/langmodels/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py:212\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparallel_apply\u001b[39m(\n\u001b[1;32m    210\u001b[0m     \u001b[38;5;28mself\u001b[39m, replicas: Sequence[T], inputs: Sequence[Any], kwargs: Any\n\u001b[1;32m    211\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Any]:\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/langmodels/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:118\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m    116\u001b[0m         thread\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m thread \u001b[38;5;129;01min\u001b[39;00m threads:\n\u001b[0;32m--> 118\u001b[0m         \u001b[43mthread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m     _worker(\u001b[38;5;241m0\u001b[39m, modules[\u001b[38;5;241m0\u001b[39m], inputs[\u001b[38;5;241m0\u001b[39m], kwargs_tup[\u001b[38;5;241m0\u001b[39m], devices[\u001b[38;5;241m0\u001b[39m], streams[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m/usr/lib/python3.12/threading.py:1147\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1144\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1147\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1149\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m/usr/lib/python3.12/threading.py:1167\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1168\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m   1169\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import BertForMaskedLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import Dataset\n",
    "\n",
    "# Convert data to Hugging Face Dataset format for training\n",
    "dataset = Dataset.from_dict({\n",
    "    \"input_ids\": [data[\"input_ids\"].squeeze(0) for data in tokenized_bpe],\n",
    "    \"attention_mask\": [data[\"attention_mask\"].squeeze(0) for data in tokenized_bpe],\n",
    "})\n",
    "\n",
    "# Use DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=mbert_tokenizerBPE,\n",
    "    mlm_probability=0.15,  # 15% masking probability\n",
    ")\n",
    "\n",
    "# Load mBERT model\n",
    "modelBPE = BertForMaskedLM.from_pretrained(\"bert-base-multilingual-uncased\")\n",
    "\n",
    "# Define training arguments, with checkpoint-saving configurations\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./mbert_BPE\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=2,  # Accumulate gradients over 2 steps\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    logging_dir='./logs',\n",
    "    fp16=True,\n",
    "    no_cuda=False  # This allows GPU if available, otherwise fall back to CPU\n",
    ")\n",
    "\n",
    "# Initialize the Trainer for fine-tuning\n",
    "trainer = Trainer(\n",
    "    model=modelBPE,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Fine-tune the model (checkpoints will automatically be saved at specified steps)\n",
    "trainer.train()\n",
    "\n",
    "# Save the final model and tokenizer\n",
    "modelBPE.save_pretrained(\"./mbert_BPE\")\n",
    "mbert_tokenizerBPE.save_pretrained(\"./mbert_BPE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/atrujillo/langmodels/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:774: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForMaskedLM, BertTokenizer\n",
    "\n",
    "# Load the trained model and tokenizer\n",
    "model = BertForMaskedLM.from_pretrained(\"./mbert_BPE\", output_hidden_states=True)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"./mbert_BPE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\" \".join(subwords) for subwords in sentences_bpe]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# obtain embeddings for words\n",
    "def get_word_embedding_with_bpe(model, tokenizer, word, bpe_tokenizer, aggregation_method=\"sum\"):\n",
    "    \"\"\"\n",
    "    Get the embedding for a single word, segmented with BPE.\n",
    "    \n",
    "    Args:\n",
    "        model: Pretrained model used for embeddings.\n",
    "        tokenizer: Tokenizer corresponding to the model.\n",
    "        word (str): The word to be embedded.\n",
    "        bpe_tokenizer: SentencePiece or BPE tokenizer for segmentation.\n",
    "        aggregation_method (str): Method to aggregate subword embeddings.\n",
    "                                  Options: \"mean\" (default), \"sum\", \"max\".\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Aggregated embedding for the input word.\n",
    "    \"\"\"\n",
    "    # Segment the word into subwords using BPE\n",
    "    bpe_subwords = bpe_tokenizer.encode(word, out_type=str)  # Segment using BPE\n",
    "    segmented_word = \" \".join(bpe_subwords)  # Join subwords into a single string\n",
    "\n",
    "    # Tokenize the segmented word for the model\n",
    "    inputs = tokenizer(segmented_word, return_tensors=\"pt\", truncation=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get model outputs\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Extract hidden states (last layer)\n",
    "    hidden_states = outputs.hidden_states[-1]  # Shape: (batch_size, seq_length, hidden_dim)\n",
    "\n",
    "    # Remove batch dimension\n",
    "    subword_embeddings = hidden_states.squeeze(0)  # Shape: (num_subwords, hidden_dim)\n",
    "    \n",
    "    # Aggregate subword embeddings\n",
    "    if aggregation_method == \"mean\":\n",
    "        word_embedding = subword_embeddings.mean(dim=0)  # Average pooling\n",
    "    elif aggregation_method == \"sum\":\n",
    "        word_embedding = subword_embeddings.sum(dim=0)  # Sum pooling\n",
    "    elif aggregation_method == \"max\":\n",
    "        word_embedding, _ = subword_embeddings.max(dim=0)  # Max pooling\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported aggregation method: {aggregation_method}\")\n",
    "    \n",
    "    return word_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "word = \"haton\"\n",
    "\n",
    "# Get embedding for the word\n",
    "embedding = get_word_embedding_with_bpe(model, tokenizer, word, bpe_tokenizer, aggregation_method=\"sum\")\n",
    "\n",
    "print(embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_words(sentences):\n",
    "    words = set()\n",
    "    for sentence in sentences:\n",
    "        words.update(sentence.strip().split())\n",
    "    return list(words)\n",
    "\n",
    "unique_words = get_unique_words(sentences)\n",
    "\n",
    "# Select 20 random words\n",
    "import random\n",
    "selected_words = random.sample(unique_words, 20)\n",
    "\n",
    "# Get embeddings\n",
    "def get_word_embeddings(words, model, tokenizer):\n",
    "    embeddings = []\n",
    "    for word in words:\n",
    "        embedding = get_word_embedding_with_bpe(model, tokenizer, word, bpe_tokenizer, aggregation_method=\"max\")  # Assuming function is defined\n",
    "        embeddings.append(embedding)\n",
    "    return embeddings\n",
    "\n",
    "word_vectors = get_word_embeddings(selected_words, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_embeddings_2D(words, vectors, dimension='tsne'):\n",
    "    \"\"\"Visualize word vectors in 2D using t-SNE or PCA.\"\"\"\n",
    "    # Reduce dimensionality using t-SNE\n",
    "    if dimension == 'tsne':\n",
    "        perplexity = min(30, len(words) - 1)  # Ensure perplexity is less than the number of samples\n",
    "        tsne = TSNE(n_components=2, perplexity=perplexity, random_state=0)\n",
    "        reduced_vectors = tsne.fit_transform(vectors)\n",
    "    # Reduce dimensionality using PCA\n",
    "    # elif dimension == 'pca':\n",
    "    #     pca = PCA(n_components=2)\n",
    "    #     reduced_vectors = pca.fit_transform(vectors)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid dimension specified. Use 'tsne' or 'pca'.\")\n",
    "\n",
    "    # Create a scatter plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(reduced_vectors[:, 0], reduced_vectors[:, 1])\n",
    "\n",
    "    # Annotate each point with the corresponding word\n",
    "    for i, word in enumerate(words):\n",
    "        plt.annotate(word, (reduced_vectors[i, 0], reduced_vectors[i, 1]), fontsize=9)\n",
    "\n",
    "    plt.title(f'VisualizaciÃ³n 2D de los vectores - mBERT ({dimension.upper()})')\n",
    "    plt.xlabel('Componente 1')\n",
    "    plt.ylabel('Componente 2')\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert word embeddings (list of tensors) to a 2D numpy array\n",
    "def embeddings_to_numpy(word_vectors):\n",
    "    # Stack tensors into a numpy array\n",
    "    return np.vstack([embedding.numpy() for embedding in word_vectors])\n",
    "\n",
    "# Convert word_vectors list into numpy array\n",
    "word_vectors_np = embeddings_to_numpy(word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_embeddings_2D(selected_words, word_vectors_np, dimension='tsne')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'penen',\n",
    " 'xoxoai',\n",
    " 'ewekasai',"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â–penen\n"
     ]
    }
   ],
   "source": [
    "embedding = get_word_embedding_with_bpe(model, tokenizer, 'penen', bpe_tokenizer, aggregation_method=\"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.5153e+00,  7.9027e-02,  2.3276e-01,  5.0219e-01, -1.2233e+00,\n",
       "         8.4892e-01,  4.2895e-01, -9.0510e-01, -1.7267e+00, -2.5598e-01,\n",
       "        -2.1635e+00,  1.3284e+00, -3.6679e-01, -1.1590e+00,  3.0493e-01,\n",
       "        -1.0857e+00,  1.5692e+00, -4.5965e-01,  4.2346e-01,  1.1239e+00,\n",
       "        -9.6299e-01,  3.5491e-01,  9.1542e-01, -8.3450e-01, -2.3617e-01,\n",
       "        -2.5464e-01, -7.6568e-01,  1.1846e+00,  3.0013e-02, -1.7850e-01,\n",
       "        -1.8626e+00, -5.1124e-01, -4.3504e-01,  3.0020e-01,  2.0993e+00,\n",
       "        -1.5453e+00, -2.0496e+00,  1.2942e+00,  1.1990e+00, -1.3655e+00,\n",
       "        -1.5062e+00,  2.7977e-02,  7.2432e-01, -3.3029e-01, -1.2466e+00,\n",
       "         1.3549e+00,  7.7492e-01,  2.6575e-01,  8.3675e-01,  6.6842e-01,\n",
       "        -1.3062e+00,  1.0206e+00,  1.0181e+00, -1.5628e+00, -8.0260e-01,\n",
       "        -2.0211e-01,  1.8361e+00, -1.1098e-04, -1.0661e+00, -4.2372e-01,\n",
       "         2.8513e+00, -1.3714e+00,  8.4333e-01,  1.7192e+00, -1.0184e+00,\n",
       "        -1.3437e+00,  1.5731e+00, -1.1144e+00, -6.6950e-01,  4.0702e-01,\n",
       "         1.3170e-01,  9.1815e-01, -2.6909e+00, -4.5219e-01,  6.0233e-01,\n",
       "        -3.2738e-01,  1.8966e-01,  1.8437e+00, -2.1189e-02,  9.4572e-01,\n",
       "        -4.4387e-01,  1.0059e+00,  2.5208e+00,  3.7720e-01, -8.9374e-01,\n",
       "        -8.3114e-01, -8.0392e-01,  1.7042e+00, -4.3475e-01,  4.9908e-01,\n",
       "         1.4850e+00,  5.6020e-01,  1.7246e+00,  4.0722e-01, -8.7076e-01,\n",
       "        -1.7496e-02, -3.2477e+00,  2.0313e+00, -4.8366e-01,  3.4388e+00,\n",
       "         2.0876e+00, -4.7705e-02, -6.3119e-01, -1.8969e+00,  5.6879e-01,\n",
       "         2.9475e-01, -1.9050e+00,  7.5816e-01,  5.8817e-01,  6.4082e-01,\n",
       "         5.9352e-01, -1.6257e+00, -6.5491e-01,  5.5867e-01, -2.4089e-02,\n",
       "         1.2752e+00,  1.6204e-01, -6.4148e-01, -3.0321e-01,  9.6095e-01,\n",
       "        -1.6788e+00,  6.3540e-01,  6.4139e-01, -6.4027e-01, -2.6407e-01,\n",
       "         1.0287e+00, -7.2458e-01,  3.5603e-01,  1.5152e+00, -2.1427e-01,\n",
       "        -2.1704e+00, -6.3470e-01,  1.2628e+00,  6.3457e-01,  1.8937e+00,\n",
       "        -8.2522e-01, -1.6955e+00,  3.6387e-02,  1.4016e-01,  1.2267e+00,\n",
       "        -1.9859e-01,  5.3177e-01,  1.0410e+00, -1.4603e+00, -1.4216e+00,\n",
       "         2.3809e-01,  6.5832e-01,  3.8004e-01,  9.1277e-02,  1.9421e+00,\n",
       "         2.6698e+00,  1.3322e+00, -6.2023e-01, -7.9065e-01, -2.9835e-01,\n",
       "         1.5127e+00,  8.8162e-02,  9.5541e-01,  9.8168e-01, -1.6649e+00,\n",
       "        -1.3250e+00,  8.7058e-01,  1.4741e+00,  3.1378e+00,  1.1252e+00,\n",
       "        -2.3705e+00, -8.1922e-03,  5.1796e-01,  7.7728e-01,  5.4964e-01,\n",
       "         2.3453e+00,  1.4711e+00, -9.0912e-01, -2.5259e+00, -8.0603e-01,\n",
       "         7.6199e-01, -1.6746e+00,  3.9893e+00, -1.2310e+00, -2.6408e-01,\n",
       "        -2.3648e-01,  4.2586e-01,  4.4140e-01,  5.9741e-01, -1.1115e+00,\n",
       "         1.5524e+00,  9.6835e-01, -1.2643e+00,  1.4603e+00, -1.7719e+00,\n",
       "        -3.2709e-01, -1.6314e+00,  9.8849e-01,  4.7407e-01, -1.0149e+00,\n",
       "        -1.6194e+00,  3.0638e-01,  7.2144e-01, -2.6713e-01,  2.0980e+00,\n",
       "         2.5871e-02, -3.6582e-01, -7.1100e-02,  7.8737e-01, -1.6258e+00,\n",
       "         4.8336e-01, -1.2268e+00, -1.0476e-01,  6.0065e-01,  1.6079e+00,\n",
       "         1.7270e+00,  3.2774e-01, -8.0868e-01, -1.7142e-01, -1.2872e+00,\n",
       "         1.0298e-01, -2.2146e+00, -8.0391e-01,  3.9708e-01, -1.0462e-01,\n",
       "        -3.8312e-01,  6.1720e-01,  1.8850e-01,  2.2357e-01, -3.9341e-01,\n",
       "         2.6570e-01, -5.1623e-01, -2.9508e-01,  2.9421e-01, -6.7561e-01,\n",
       "         2.6437e-01,  7.5594e-01,  2.8327e-01, -1.6081e+00,  9.0869e-01,\n",
       "        -1.1841e+00,  1.4492e-01, -7.2146e-01,  1.4255e-01,  6.1271e-01,\n",
       "         2.4523e-01, -2.9433e+00,  2.1364e+00,  1.1992e+00,  3.6500e-01,\n",
       "         1.9236e+00, -7.8407e-01, -6.9170e-01, -1.4585e+00,  2.2542e-02,\n",
       "         4.4012e-01, -8.5718e-01, -1.4533e+00,  1.9162e+00,  8.9887e-01,\n",
       "         6.5162e-01,  2.1413e+00, -1.0303e+00,  1.3952e+00, -1.6569e-02,\n",
       "        -2.2876e+00,  4.4326e-01, -1.3505e+00,  7.1150e-01,  1.8055e+00,\n",
       "        -1.1345e+00,  1.5641e-01,  1.4543e-01, -1.8595e+00,  2.8483e-01,\n",
       "         2.4809e-02,  3.7809e-01,  6.7778e-01,  3.9231e-01,  7.8879e-02,\n",
       "        -1.3849e+00, -1.6327e+00,  4.0974e-01,  5.6689e-01, -1.9085e+00,\n",
       "        -8.2378e-01,  1.3025e+00,  1.3466e+00,  7.7497e-01,  5.9033e-01,\n",
       "        -4.5209e-03,  8.2825e-01, -7.7769e-01,  8.6266e-01, -1.2725e+00,\n",
       "        -1.1099e+00,  5.7938e-01,  1.8283e+00,  1.1288e-01,  3.6022e-01,\n",
       "        -1.2152e+00,  7.4425e-01,  1.3016e+00, -6.8061e-01,  1.0312e+00,\n",
       "         9.4644e-01, -7.9745e-01,  1.0609e-01, -6.4978e-01, -2.4883e-01,\n",
       "         5.2072e-01, -1.0228e+00,  4.6520e-01, -2.5164e-01,  2.0270e-02,\n",
       "        -1.2389e+00, -1.3698e+00,  6.3027e-01, -1.3492e+00, -2.9408e-01,\n",
       "        -1.6755e+00, -1.5078e+00, -1.0490e+00, -1.4211e+00, -5.3608e-01,\n",
       "         3.7896e-01, -1.7671e+00,  1.0348e+00, -1.3347e+00, -5.6374e-01,\n",
       "         1.6911e+00, -7.8498e-01, -6.0146e-01,  1.0418e+00, -6.3011e-01,\n",
       "        -6.5595e-01, -6.1327e-02, -4.7111e-01,  8.0652e-01,  1.5620e+00,\n",
       "         7.6739e-01, -1.0646e+00,  2.6683e-01,  4.6320e-02,  7.8023e-03,\n",
       "        -4.4430e-01, -7.3762e-01,  1.6301e+00,  2.0041e-02, -1.6855e-01,\n",
       "        -6.4785e-01, -1.0755e+00,  7.1466e-01, -4.5888e-01,  9.8369e-01,\n",
       "        -2.7787e+00,  7.1492e-01,  3.7486e-01, -1.4603e+00, -5.4790e-01,\n",
       "        -2.4681e+00, -8.4242e-01,  3.9101e-01, -1.2767e+00,  2.3806e+00,\n",
       "        -2.9991e-01,  3.1396e+00, -2.8940e-02,  2.4354e-01,  1.1462e-01,\n",
       "        -4.0042e-01, -5.6181e-02,  4.8919e-01,  2.8275e-01, -1.4145e+00,\n",
       "         2.4548e-01,  4.5677e-01,  2.0370e+00,  2.9995e-01,  1.8374e-02,\n",
       "        -6.0111e-01,  7.3374e-01,  2.1672e+00, -1.9361e-01, -5.8089e-01,\n",
       "         7.6250e-01, -2.4074e-01, -1.5200e-01, -8.2686e-01,  6.0723e-01,\n",
       "        -3.6298e-01, -2.1114e+00, -2.4652e+00, -4.3291e-01, -1.0485e-01,\n",
       "        -1.1242e-01,  1.1603e+00, -2.9809e+00, -2.2575e-01, -4.2103e-01,\n",
       "         7.0651e-01, -9.7613e-01, -1.6064e+00,  5.4466e-01,  1.7366e+00,\n",
       "         4.5016e-02,  9.9673e-01, -3.9275e-01,  2.7177e-01,  2.2116e-01,\n",
       "        -3.1894e-01,  2.6669e-01, -6.1630e-02,  3.4812e-01, -4.9330e-01,\n",
       "        -3.2927e+00,  1.3155e+00, -2.1657e-01, -6.2757e+00,  1.9802e+00,\n",
       "         1.6053e+00,  9.3731e-02, -2.0573e-01, -6.6630e-01,  1.9238e-01,\n",
       "         1.6160e+00, -1.9099e+00, -1.0010e+00, -1.7911e+00,  5.4803e-01,\n",
       "        -9.6836e-01, -1.9733e+00,  5.8628e-01,  1.2589e+00,  1.2192e+00,\n",
       "        -2.1685e-01,  2.1706e+00, -6.4220e-01,  1.1609e-01, -1.5665e+00,\n",
       "        -1.6725e-01,  4.2234e-01, -3.1189e-01, -6.6792e-01,  4.3225e-01,\n",
       "         1.2195e+00, -4.1554e-01,  4.1963e-01, -2.6225e-01, -4.2144e-01,\n",
       "        -4.2837e-02,  2.4292e+00,  5.7853e-01,  4.8599e-01,  5.0313e-01,\n",
       "        -9.7433e-01, -6.5434e-01, -6.0187e-01,  1.0290e+00, -6.8097e-01,\n",
       "         2.0212e-01, -1.9012e+00,  1.2451e+00,  1.1217e+00,  2.2463e+00,\n",
       "        -9.9260e-01,  6.8870e-01,  6.2349e-01,  3.8008e-01, -1.5651e+00,\n",
       "        -1.8341e+00, -6.7699e-01, -1.1437e+00,  1.3638e+00, -2.4043e+00,\n",
       "        -6.8626e-01,  1.1216e+00, -1.2150e+00, -7.2887e-01, -7.3982e-01,\n",
       "         1.6231e+00,  7.1648e-01,  1.1171e-01,  2.1930e+00, -1.2066e+00,\n",
       "        -7.4817e-01, -1.0102e+00,  2.1487e-01,  1.5120e+00,  1.9354e+00,\n",
       "        -7.2393e-01, -1.9953e-01, -3.6748e-02,  1.2315e+00,  2.5282e-01,\n",
       "        -8.6758e-01, -1.4620e+00, -4.2234e-01,  2.4398e-01, -1.5650e+00,\n",
       "        -3.6865e-01, -3.0559e-02,  2.1647e+00, -9.0758e-01,  3.8720e-01,\n",
       "        -2.2449e-02,  8.5988e-01, -5.4331e-01,  1.9943e-02,  2.6942e-01,\n",
       "        -1.3354e+00,  1.1439e+00, -3.8571e-02, -1.5208e+00,  1.0122e+00,\n",
       "        -4.8512e-01,  3.9664e-01, -6.9623e-01,  2.0592e+00, -1.5385e+00,\n",
       "        -8.9846e-01, -2.5786e-02, -1.2530e+00, -4.5052e-01, -7.8082e-01,\n",
       "        -2.0691e-01, -1.7010e+00,  1.8722e+00,  5.4448e-01, -9.2262e-01,\n",
       "        -8.4348e-01,  5.2582e-01, -1.2856e+00, -2.3785e-02, -4.5013e-01,\n",
       "        -1.2750e+00, -8.2543e-01, -2.2799e-01,  9.3951e-01,  2.7002e+00,\n",
       "         4.9380e-01, -2.9153e-01,  3.8728e-01,  7.2510e-01,  4.1744e-01,\n",
       "        -7.3129e-01, -6.5553e-01, -2.3434e+00,  2.0235e+00,  6.9189e-01,\n",
       "         2.0339e+00, -8.2539e-01,  1.1865e+00, -3.1782e-02, -6.1196e-01,\n",
       "         3.2527e-01,  1.7672e+00,  1.3477e+00,  4.9095e-01,  5.4865e-01,\n",
       "        -3.0832e-01,  6.8134e-01, -5.0456e-01, -7.6331e-01, -2.1663e-01,\n",
       "        -1.1864e+00,  1.3061e+00,  2.4866e-01, -2.4093e+00, -7.2240e-01,\n",
       "         8.2600e-01,  1.8143e+00,  1.2178e+00, -4.9837e-01, -2.7453e+00,\n",
       "        -5.5059e-01, -2.0596e-01, -1.8802e+00,  1.0131e+00,  8.0586e-01,\n",
       "         2.1731e+00,  2.2265e-01, -6.8402e-01,  1.6092e-02, -1.9360e+00,\n",
       "        -1.3771e+00, -1.0943e-01, -8.7086e-01,  1.0814e+00,  1.5563e+00,\n",
       "         9.5886e-01, -1.1859e+00,  6.7664e-01,  2.6633e-01, -1.3383e+00,\n",
       "        -6.7502e-01,  3.8252e-01,  1.0305e+00, -2.0759e+00,  1.0520e+00,\n",
       "         4.3716e-01,  5.4479e-01, -7.7154e-02,  1.2043e+00,  4.6122e-01,\n",
       "         2.5389e-02,  6.1421e-01,  1.7500e+00,  2.1427e+00,  3.9853e-01,\n",
       "         5.3675e-01,  1.1022e+00,  9.0272e-02, -5.1603e-02, -9.7088e-01,\n",
       "         2.9755e-01, -1.3244e-01, -1.7798e+00,  6.7096e-01, -1.4988e+00,\n",
       "        -1.3018e+00,  1.1535e-01, -5.6314e-03,  2.3834e+00, -4.4242e-01,\n",
       "         2.3524e+00, -1.7489e-01, -6.9446e-01,  1.7999e+00, -5.7189e-01,\n",
       "        -8.6432e-01, -1.1043e-01, -1.9913e-01,  4.9369e-01,  6.3871e-03,\n",
       "         3.6636e-01, -2.7364e-01, -1.4179e+00,  1.4503e+00, -6.5686e-01,\n",
       "         3.9667e-01,  1.7615e+00,  9.2093e-01, -8.5004e-02,  8.1512e-01,\n",
       "         1.1484e+00,  2.2065e+00, -1.6917e+00,  2.2642e+00,  1.0708e+00,\n",
       "         9.6647e-02,  1.8225e+00,  7.4252e-01, -6.5981e-01, -1.6103e-01,\n",
       "        -1.7671e+00,  1.1960e-01, -4.7455e-01, -1.8538e-01,  3.7634e-01,\n",
       "        -7.8867e-01, -4.6954e-01, -9.9163e-01,  1.3245e+00,  9.4312e-01,\n",
       "        -3.9685e-02,  7.4513e-01, -1.5707e+00, -2.0826e+00,  4.4930e-01,\n",
       "        -3.0082e-01, -4.0882e-01,  1.2709e+00,  2.0518e+00, -1.2635e+00,\n",
       "        -1.1965e+00, -3.5537e+00, -1.0637e+00,  4.1269e-01, -1.1104e-01,\n",
       "        -1.0536e+00,  1.5836e+00, -6.4692e-02, -1.0315e-01, -7.6508e-01,\n",
       "         2.2053e-01,  1.2595e+00,  2.3300e+00,  9.4477e-01,  1.7819e+00,\n",
       "        -4.8260e-01,  9.8327e-01,  7.9237e-02, -6.4907e-01,  2.2273e+00,\n",
       "        -9.4577e-01, -1.5114e-01, -1.8602e-01, -1.5034e+00,  4.0253e-01,\n",
       "         2.0000e-01, -8.7629e-02, -1.4608e+00, -2.3224e-01,  2.7129e-01,\n",
       "         3.3354e+00,  2.1365e+00, -9.8744e-01, -1.0084e+00,  4.8614e-01,\n",
       "         4.1996e-01,  1.9468e+00, -1.5393e+00, -8.9710e-01, -7.0239e-01,\n",
       "         1.3596e+00, -2.3678e-01, -1.7946e+00, -7.7087e-01, -4.6480e-01,\n",
       "        -3.9035e-01, -5.5964e-01,  1.4483e+00, -2.9438e-01, -1.7290e-01,\n",
       "        -1.0114e-01, -1.7082e+00,  1.1194e+00,  1.1756e+00, -1.9267e+00,\n",
       "         8.0419e-01,  1.7878e+00,  2.1258e+00, -1.3908e+00,  1.4482e+00,\n",
       "         2.0742e-01, -1.3729e-01, -2.0497e-01, -2.4328e-01,  9.6933e-02,\n",
       "         1.3315e-01,  8.8646e-01, -9.9573e-01,  1.8193e+00,  1.0522e+00,\n",
       "         1.6777e+00, -1.7973e+00, -1.4897e+00, -1.8711e+00,  1.6881e-01,\n",
       "         8.8115e-01, -5.5090e-01,  2.1281e-01,  2.4886e-01, -4.0655e-01,\n",
       "        -1.3304e+00,  5.1598e-01, -1.2807e+00, -5.4001e-01,  1.2486e+00,\n",
       "        -4.1583e-02,  5.4429e-01,  2.3982e-01,  4.3237e-01, -6.3696e-01,\n",
       "        -9.3107e-01, -1.6405e+00, -1.2350e+00, -3.0175e+00, -2.3838e+00,\n",
       "         1.0977e+00,  2.6330e+00, -1.1052e+00])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.5153e+00,  7.9027e-02,  2.3276e-01,  5.0219e-01, -1.2233e+00,\n",
       "         8.4892e-01,  4.2895e-01, -9.0510e-01, -1.7267e+00, -2.5598e-01,\n",
       "        -2.1635e+00,  1.3284e+00, -3.6679e-01, -1.1590e+00,  3.0493e-01,\n",
       "        -1.0857e+00,  1.5692e+00, -4.5965e-01,  4.2346e-01,  1.1239e+00,\n",
       "        -9.6299e-01,  3.5491e-01,  9.1542e-01, -8.3450e-01, -2.3617e-01,\n",
       "        -2.5464e-01, -7.6568e-01,  1.1846e+00,  3.0013e-02, -1.7850e-01,\n",
       "        -1.8626e+00, -5.1124e-01, -4.3504e-01,  3.0020e-01,  2.0993e+00,\n",
       "        -1.5453e+00, -2.0496e+00,  1.2942e+00,  1.1990e+00, -1.3655e+00,\n",
       "        -1.5062e+00,  2.7977e-02,  7.2432e-01, -3.3029e-01, -1.2466e+00,\n",
       "         1.3549e+00,  7.7492e-01,  2.6575e-01,  8.3675e-01,  6.6842e-01,\n",
       "        -1.3062e+00,  1.0206e+00,  1.0181e+00, -1.5628e+00, -8.0260e-01,\n",
       "        -2.0211e-01,  1.8361e+00, -1.1098e-04, -1.0661e+00, -4.2372e-01,\n",
       "         2.8513e+00, -1.3714e+00,  8.4333e-01,  1.7192e+00, -1.0184e+00,\n",
       "        -1.3437e+00,  1.5731e+00, -1.1144e+00, -6.6950e-01,  4.0702e-01,\n",
       "         1.3170e-01,  9.1815e-01, -2.6909e+00, -4.5219e-01,  6.0233e-01,\n",
       "        -3.2738e-01,  1.8966e-01,  1.8437e+00, -2.1189e-02,  9.4572e-01,\n",
       "        -4.4387e-01,  1.0059e+00,  2.5208e+00,  3.7720e-01, -8.9374e-01,\n",
       "        -8.3114e-01, -8.0392e-01,  1.7042e+00, -4.3475e-01,  4.9908e-01,\n",
       "         1.4850e+00,  5.6020e-01,  1.7246e+00,  4.0722e-01, -8.7076e-01,\n",
       "        -1.7496e-02, -3.2477e+00,  2.0313e+00, -4.8366e-01,  3.4388e+00,\n",
       "         2.0876e+00, -4.7705e-02, -6.3119e-01, -1.8969e+00,  5.6879e-01,\n",
       "         2.9475e-01, -1.9050e+00,  7.5816e-01,  5.8817e-01,  6.4082e-01,\n",
       "         5.9352e-01, -1.6257e+00, -6.5491e-01,  5.5867e-01, -2.4089e-02,\n",
       "         1.2752e+00,  1.6204e-01, -6.4148e-01, -3.0321e-01,  9.6095e-01,\n",
       "        -1.6788e+00,  6.3540e-01,  6.4139e-01, -6.4027e-01, -2.6407e-01,\n",
       "         1.0287e+00, -7.2458e-01,  3.5603e-01,  1.5152e+00, -2.1427e-01,\n",
       "        -2.1704e+00, -6.3470e-01,  1.2628e+00,  6.3457e-01,  1.8937e+00,\n",
       "        -8.2522e-01, -1.6955e+00,  3.6387e-02,  1.4016e-01,  1.2267e+00,\n",
       "        -1.9859e-01,  5.3177e-01,  1.0410e+00, -1.4603e+00, -1.4216e+00,\n",
       "         2.3809e-01,  6.5832e-01,  3.8004e-01,  9.1277e-02,  1.9421e+00,\n",
       "         2.6698e+00,  1.3322e+00, -6.2023e-01, -7.9065e-01, -2.9835e-01,\n",
       "         1.5127e+00,  8.8162e-02,  9.5541e-01,  9.8168e-01, -1.6649e+00,\n",
       "        -1.3250e+00,  8.7058e-01,  1.4741e+00,  3.1378e+00,  1.1252e+00,\n",
       "        -2.3705e+00, -8.1922e-03,  5.1796e-01,  7.7728e-01,  5.4964e-01,\n",
       "         2.3453e+00,  1.4711e+00, -9.0912e-01, -2.5259e+00, -8.0603e-01,\n",
       "         7.6199e-01, -1.6746e+00,  3.9893e+00, -1.2310e+00, -2.6408e-01,\n",
       "        -2.3648e-01,  4.2586e-01,  4.4140e-01,  5.9741e-01, -1.1115e+00,\n",
       "         1.5524e+00,  9.6835e-01, -1.2643e+00,  1.4603e+00, -1.7719e+00,\n",
       "        -3.2709e-01, -1.6314e+00,  9.8849e-01,  4.7407e-01, -1.0149e+00,\n",
       "        -1.6194e+00,  3.0638e-01,  7.2144e-01, -2.6713e-01,  2.0980e+00,\n",
       "         2.5871e-02, -3.6582e-01, -7.1100e-02,  7.8737e-01, -1.6258e+00,\n",
       "         4.8336e-01, -1.2268e+00, -1.0476e-01,  6.0065e-01,  1.6079e+00,\n",
       "         1.7270e+00,  3.2774e-01, -8.0868e-01, -1.7142e-01, -1.2872e+00,\n",
       "         1.0298e-01, -2.2146e+00, -8.0391e-01,  3.9708e-01, -1.0462e-01,\n",
       "        -3.8312e-01,  6.1720e-01,  1.8850e-01,  2.2357e-01, -3.9341e-01,\n",
       "         2.6570e-01, -5.1623e-01, -2.9508e-01,  2.9421e-01, -6.7561e-01,\n",
       "         2.6437e-01,  7.5594e-01,  2.8327e-01, -1.6081e+00,  9.0869e-01,\n",
       "        -1.1841e+00,  1.4492e-01, -7.2146e-01,  1.4255e-01,  6.1271e-01,\n",
       "         2.4523e-01, -2.9433e+00,  2.1364e+00,  1.1992e+00,  3.6500e-01,\n",
       "         1.9236e+00, -7.8407e-01, -6.9170e-01, -1.4585e+00,  2.2542e-02,\n",
       "         4.4012e-01, -8.5718e-01, -1.4533e+00,  1.9162e+00,  8.9887e-01,\n",
       "         6.5162e-01,  2.1413e+00, -1.0303e+00,  1.3952e+00, -1.6569e-02,\n",
       "        -2.2876e+00,  4.4326e-01, -1.3505e+00,  7.1150e-01,  1.8055e+00,\n",
       "        -1.1345e+00,  1.5641e-01,  1.4543e-01, -1.8595e+00,  2.8483e-01,\n",
       "         2.4809e-02,  3.7809e-01,  6.7778e-01,  3.9231e-01,  7.8879e-02,\n",
       "        -1.3849e+00, -1.6327e+00,  4.0974e-01,  5.6689e-01, -1.9085e+00,\n",
       "        -8.2378e-01,  1.3025e+00,  1.3466e+00,  7.7497e-01,  5.9033e-01,\n",
       "        -4.5209e-03,  8.2825e-01, -7.7769e-01,  8.6266e-01, -1.2725e+00,\n",
       "        -1.1099e+00,  5.7938e-01,  1.8283e+00,  1.1288e-01,  3.6022e-01,\n",
       "        -1.2152e+00,  7.4425e-01,  1.3016e+00, -6.8061e-01,  1.0312e+00,\n",
       "         9.4644e-01, -7.9745e-01,  1.0609e-01, -6.4978e-01, -2.4883e-01,\n",
       "         5.2072e-01, -1.0228e+00,  4.6520e-01, -2.5164e-01,  2.0270e-02,\n",
       "        -1.2389e+00, -1.3698e+00,  6.3027e-01, -1.3492e+00, -2.9408e-01,\n",
       "        -1.6755e+00, -1.5078e+00, -1.0490e+00, -1.4211e+00, -5.3608e-01,\n",
       "         3.7896e-01, -1.7671e+00,  1.0348e+00, -1.3347e+00, -5.6374e-01,\n",
       "         1.6911e+00, -7.8498e-01, -6.0146e-01,  1.0418e+00, -6.3011e-01,\n",
       "        -6.5595e-01, -6.1327e-02, -4.7111e-01,  8.0652e-01,  1.5620e+00,\n",
       "         7.6739e-01, -1.0646e+00,  2.6683e-01,  4.6320e-02,  7.8023e-03,\n",
       "        -4.4430e-01, -7.3762e-01,  1.6301e+00,  2.0041e-02, -1.6855e-01,\n",
       "        -6.4785e-01, -1.0755e+00,  7.1466e-01, -4.5888e-01,  9.8369e-01,\n",
       "        -2.7787e+00,  7.1492e-01,  3.7486e-01, -1.4603e+00, -5.4790e-01,\n",
       "        -2.4681e+00, -8.4242e-01,  3.9101e-01, -1.2767e+00,  2.3806e+00,\n",
       "        -2.9991e-01,  3.1396e+00, -2.8940e-02,  2.4354e-01,  1.1462e-01,\n",
       "        -4.0042e-01, -5.6181e-02,  4.8919e-01,  2.8275e-01, -1.4145e+00,\n",
       "         2.4548e-01,  4.5677e-01,  2.0370e+00,  2.9995e-01,  1.8374e-02,\n",
       "        -6.0111e-01,  7.3374e-01,  2.1672e+00, -1.9361e-01, -5.8089e-01,\n",
       "         7.6250e-01, -2.4074e-01, -1.5200e-01, -8.2686e-01,  6.0723e-01,\n",
       "        -3.6298e-01, -2.1114e+00, -2.4652e+00, -4.3291e-01, -1.0485e-01,\n",
       "        -1.1242e-01,  1.1603e+00, -2.9809e+00, -2.2575e-01, -4.2103e-01,\n",
       "         7.0651e-01, -9.7613e-01, -1.6064e+00,  5.4466e-01,  1.7366e+00,\n",
       "         4.5016e-02,  9.9673e-01, -3.9275e-01,  2.7177e-01,  2.2116e-01,\n",
       "        -3.1894e-01,  2.6669e-01, -6.1630e-02,  3.4812e-01, -4.9330e-01,\n",
       "        -3.2927e+00,  1.3155e+00, -2.1657e-01, -6.2757e+00,  1.9802e+00,\n",
       "         1.6053e+00,  9.3731e-02, -2.0573e-01, -6.6630e-01,  1.9238e-01,\n",
       "         1.6160e+00, -1.9099e+00, -1.0010e+00, -1.7911e+00,  5.4803e-01,\n",
       "        -9.6836e-01, -1.9733e+00,  5.8628e-01,  1.2589e+00,  1.2192e+00,\n",
       "        -2.1685e-01,  2.1706e+00, -6.4220e-01,  1.1609e-01, -1.5665e+00,\n",
       "        -1.6725e-01,  4.2234e-01, -3.1189e-01, -6.6792e-01,  4.3225e-01,\n",
       "         1.2195e+00, -4.1554e-01,  4.1963e-01, -2.6225e-01, -4.2144e-01,\n",
       "        -4.2837e-02,  2.4292e+00,  5.7853e-01,  4.8599e-01,  5.0313e-01,\n",
       "        -9.7433e-01, -6.5434e-01, -6.0187e-01,  1.0290e+00, -6.8097e-01,\n",
       "         2.0212e-01, -1.9012e+00,  1.2451e+00,  1.1217e+00,  2.2463e+00,\n",
       "        -9.9260e-01,  6.8870e-01,  6.2349e-01,  3.8008e-01, -1.5651e+00,\n",
       "        -1.8341e+00, -6.7699e-01, -1.1437e+00,  1.3638e+00, -2.4043e+00,\n",
       "        -6.8626e-01,  1.1216e+00, -1.2150e+00, -7.2887e-01, -7.3982e-01,\n",
       "         1.6231e+00,  7.1648e-01,  1.1171e-01,  2.1930e+00, -1.2066e+00,\n",
       "        -7.4817e-01, -1.0102e+00,  2.1487e-01,  1.5120e+00,  1.9354e+00,\n",
       "        -7.2393e-01, -1.9953e-01, -3.6748e-02,  1.2315e+00,  2.5282e-01,\n",
       "        -8.6758e-01, -1.4620e+00, -4.2234e-01,  2.4398e-01, -1.5650e+00,\n",
       "        -3.6865e-01, -3.0559e-02,  2.1647e+00, -9.0758e-01,  3.8720e-01,\n",
       "        -2.2449e-02,  8.5988e-01, -5.4331e-01,  1.9943e-02,  2.6942e-01,\n",
       "        -1.3354e+00,  1.1439e+00, -3.8571e-02, -1.5208e+00,  1.0122e+00,\n",
       "        -4.8512e-01,  3.9664e-01, -6.9623e-01,  2.0592e+00, -1.5385e+00,\n",
       "        -8.9846e-01, -2.5786e-02, -1.2530e+00, -4.5052e-01, -7.8082e-01,\n",
       "        -2.0691e-01, -1.7010e+00,  1.8722e+00,  5.4448e-01, -9.2262e-01,\n",
       "        -8.4348e-01,  5.2582e-01, -1.2856e+00, -2.3785e-02, -4.5013e-01,\n",
       "        -1.2750e+00, -8.2543e-01, -2.2799e-01,  9.3951e-01,  2.7002e+00,\n",
       "         4.9380e-01, -2.9153e-01,  3.8728e-01,  7.2510e-01,  4.1744e-01,\n",
       "        -7.3129e-01, -6.5553e-01, -2.3434e+00,  2.0235e+00,  6.9189e-01,\n",
       "         2.0339e+00, -8.2539e-01,  1.1865e+00, -3.1782e-02, -6.1196e-01,\n",
       "         3.2527e-01,  1.7672e+00,  1.3477e+00,  4.9095e-01,  5.4865e-01,\n",
       "        -3.0832e-01,  6.8134e-01, -5.0456e-01, -7.6331e-01, -2.1663e-01,\n",
       "        -1.1864e+00,  1.3061e+00,  2.4866e-01, -2.4093e+00, -7.2240e-01,\n",
       "         8.2600e-01,  1.8143e+00,  1.2178e+00, -4.9837e-01, -2.7453e+00,\n",
       "        -5.5059e-01, -2.0596e-01, -1.8802e+00,  1.0131e+00,  8.0586e-01,\n",
       "         2.1731e+00,  2.2265e-01, -6.8402e-01,  1.6092e-02, -1.9360e+00,\n",
       "        -1.3771e+00, -1.0943e-01, -8.7086e-01,  1.0814e+00,  1.5563e+00,\n",
       "         9.5886e-01, -1.1859e+00,  6.7664e-01,  2.6633e-01, -1.3383e+00,\n",
       "        -6.7502e-01,  3.8252e-01,  1.0305e+00, -2.0759e+00,  1.0520e+00,\n",
       "         4.3716e-01,  5.4479e-01, -7.7154e-02,  1.2043e+00,  4.6122e-01,\n",
       "         2.5389e-02,  6.1421e-01,  1.7500e+00,  2.1427e+00,  3.9853e-01,\n",
       "         5.3675e-01,  1.1022e+00,  9.0272e-02, -5.1603e-02, -9.7088e-01,\n",
       "         2.9755e-01, -1.3244e-01, -1.7798e+00,  6.7096e-01, -1.4988e+00,\n",
       "        -1.3018e+00,  1.1535e-01, -5.6314e-03,  2.3834e+00, -4.4242e-01,\n",
       "         2.3524e+00, -1.7489e-01, -6.9446e-01,  1.7999e+00, -5.7189e-01,\n",
       "        -8.6432e-01, -1.1043e-01, -1.9913e-01,  4.9369e-01,  6.3871e-03,\n",
       "         3.6636e-01, -2.7364e-01, -1.4179e+00,  1.4503e+00, -6.5686e-01,\n",
       "         3.9667e-01,  1.7615e+00,  9.2093e-01, -8.5004e-02,  8.1512e-01,\n",
       "         1.1484e+00,  2.2065e+00, -1.6917e+00,  2.2642e+00,  1.0708e+00,\n",
       "         9.6647e-02,  1.8225e+00,  7.4252e-01, -6.5981e-01, -1.6103e-01,\n",
       "        -1.7671e+00,  1.1960e-01, -4.7455e-01, -1.8538e-01,  3.7634e-01,\n",
       "        -7.8867e-01, -4.6954e-01, -9.9163e-01,  1.3245e+00,  9.4312e-01,\n",
       "        -3.9685e-02,  7.4513e-01, -1.5707e+00, -2.0826e+00,  4.4930e-01,\n",
       "        -3.0082e-01, -4.0882e-01,  1.2709e+00,  2.0518e+00, -1.2635e+00,\n",
       "        -1.1965e+00, -3.5537e+00, -1.0637e+00,  4.1269e-01, -1.1104e-01,\n",
       "        -1.0536e+00,  1.5836e+00, -6.4692e-02, -1.0315e-01, -7.6508e-01,\n",
       "         2.2053e-01,  1.2595e+00,  2.3300e+00,  9.4477e-01,  1.7819e+00,\n",
       "        -4.8260e-01,  9.8327e-01,  7.9237e-02, -6.4907e-01,  2.2273e+00,\n",
       "        -9.4577e-01, -1.5114e-01, -1.8602e-01, -1.5034e+00,  4.0253e-01,\n",
       "         2.0000e-01, -8.7629e-02, -1.4608e+00, -2.3224e-01,  2.7129e-01,\n",
       "         3.3354e+00,  2.1365e+00, -9.8744e-01, -1.0084e+00,  4.8614e-01,\n",
       "         4.1996e-01,  1.9468e+00, -1.5393e+00, -8.9710e-01, -7.0239e-01,\n",
       "         1.3596e+00, -2.3678e-01, -1.7946e+00, -7.7087e-01, -4.6480e-01,\n",
       "        -3.9035e-01, -5.5964e-01,  1.4483e+00, -2.9438e-01, -1.7290e-01,\n",
       "        -1.0114e-01, -1.7082e+00,  1.1194e+00,  1.1756e+00, -1.9267e+00,\n",
       "         8.0419e-01,  1.7878e+00,  2.1258e+00, -1.3908e+00,  1.4482e+00,\n",
       "         2.0742e-01, -1.3729e-01, -2.0497e-01, -2.4328e-01,  9.6933e-02,\n",
       "         1.3315e-01,  8.8646e-01, -9.9573e-01,  1.8193e+00,  1.0522e+00,\n",
       "         1.6777e+00, -1.7973e+00, -1.4897e+00, -1.8711e+00,  1.6881e-01,\n",
       "         8.8115e-01, -5.5090e-01,  2.1281e-01,  2.4886e-01, -4.0655e-01,\n",
       "        -1.3304e+00,  5.1598e-01, -1.2807e+00, -5.4001e-01,  1.2486e+00,\n",
       "        -4.1583e-02,  5.4429e-01,  2.3982e-01,  4.3237e-01, -6.3696e-01,\n",
       "        -9.3107e-01, -1.6405e+00, -1.2350e+00, -3.0175e+00, -2.3838e+00,\n",
       "         1.0977e+00,  2.6330e+00, -1.1052e+00])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â–akan a\n"
     ]
    }
   ],
   "source": [
    "embedding2 = get_word_embedding_with_bpe(model, tokenizer, 'akana', bpe_tokenizer, aggregation_method=\"sum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ashinka',\n",
       " 'merahonka',\n",
       " 'akana',\n",
       " 'hai',\n",
       " 'hatian',\n",
       " 'pakaronin',\n",
       " 'chokama',\n",
       " 'ikiribi',\n",
       " 'bakomara',\n",
       " 'Ã©l',\n",
       " 'askahon',\n",
       " 'okekairi',\n",
       " 'kapohon',\n",
       " 'mahkanike',\n",
       " 'materi',\n",
       " 'mapehaki',\n",
       " 'wewamisi',\n",
       " 'hacha',\n",
       " 'kake',\n",
       " 'onon']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/atrujillo/langmodels/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1539' max='1539' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1539/1539 06:08, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>4.052300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.801400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.937000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/atrujillo/langmodels/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/atrujillo/langmodels/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/home/atrujillo/langmodels/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Trainer: evaluation requires an eval_dataset.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Step 7: Train and evaluate the model\u001b[39;00m\n\u001b[1;32m     42\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 43\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# This evaluates on test_dataset\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Step 8: Save the final model and tokenizer\u001b[39;00m\n\u001b[1;32m     46\u001b[0m modelBPE\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./mbert_BPE_t\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/langmodels/lib/python3.12/site-packages/transformers/trainer.py:3968\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3965\u001b[0m \u001b[38;5;66;03m# memory metrics - must set up as early as possible\u001b[39;00m\n\u001b[1;32m   3966\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_memory_tracker\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m-> 3968\u001b[0m eval_dataloader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_eval_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3969\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_fsdp_xla_v2_enabled:\n\u001b[1;32m   3970\u001b[0m     eval_dataloader \u001b[38;5;241m=\u001b[39m tpu_spmd_dataloader(eval_dataloader)\n",
      "File \u001b[0;32m~/langmodels/lib/python3.12/site-packages/transformers/trainer.py:1032\u001b[0m, in \u001b[0;36mTrainer.get_eval_dataloader\u001b[0;34m(self, eval_dataset)\u001b[0m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;124;03mReturns the evaluation [`~torch.utils.data.DataLoader`].\u001b[39;00m\n\u001b[1;32m   1024\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;124;03m        If a `str`, will use `self.eval_dataset[eval_dataset]` as the evaluation dataset. If a `Dataset`, will override `self.eval_dataset` and must implement `__len__`. If it is a [`~datasets.Dataset`], columns not accepted by the `model.forward()` method are automatically removed.\u001b[39;00m\n\u001b[1;32m   1030\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m eval_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1032\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrainer: evaluation requires an eval_dataset.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;66;03m# If we have persistent workers, don't do a fork bomb especially as eval datasets\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;66;03m# don't change during training\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m dataloader_key \u001b[38;5;241m=\u001b[39m eval_dataset \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(eval_dataset, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: Trainer: evaluation requires an eval_dataset."
     ]
    }
   ],
   "source": [
    "from transformers import BertForMaskedLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "\n",
    "# Step 2: Convert train and test data to Hugging Face Dataset format\n",
    "train_dataset = Dataset.from_dict({\n",
    "    \"input_ids\": [data[\"input_ids\"].squeeze(0) for data in tokenized_bpe],\n",
    "    \"attention_mask\": [data[\"attention_mask\"].squeeze(0) for data in tokenized_bpe],\n",
    "})\n",
    "# Step 3: Use DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=mbert_tokenizerBPE,\n",
    "    mlm_probability=0.15,\n",
    ")\n",
    "\n",
    "# Step 4: Load mBERT model\n",
    "modelBPE = BertForMaskedLM.from_pretrained(\"bert-base-multilingual-uncased\")\n",
    "\n",
    "# Step 5: Define training arguments with evaluation\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./mbert_BPE_t\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=2,  # Accumulate gradients over 2 steps\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    logging_dir='./logs',\n",
    "    fp16=True,\n",
    "    no_cuda=False,\n",
    ")\n",
    "\n",
    "# Step 6: Initialize Trainer for fine-tuning with eval_dataset\n",
    "trainer = Trainer(\n",
    "    model=modelBPE,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Step 7: Train and evaluate the model\n",
    "trainer.train()\n",
    "\n",
    "# Step 8: Save the final model and tokenizer\n",
    "modelBPE.save_pretrained(\"./mbert_BPE_t\")\n",
    "mbert_tokenizerBPE.save_pretrained(\"./mbert_BPE_t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./mbert_BPE_t/tokenizer_config.json',\n",
       " './mbert_BPE_t/special_tokens_map.json',\n",
       " './mbert_BPE_t/vocab.txt',\n",
       " './mbert_BPE_t/added_tokens.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelBPE.save_pretrained(\"./mbert_BPE_t\")\n",
    "mbert_tokenizerBPE.save_pretrained(\"./mbert_BPE_t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected format for prediction at index 0: [{'score': 0.0659668892621994, 'token': 10237, 'token_str': 'ka', 'sequence': '[CLS] ka [UNK] paoni ka [UNK] [UNK] ho koa [UNK] [MASK] bi [UNK] [UNK] ma kana [UNK] [UNK] [SEP]'}, {'score': 0.05690357834100723, 'token': 10863, 'token_str': 'bi', 'sequence': '[CLS] bi [UNK] paoni ka [UNK] [UNK] ho koa [UNK] [MASK] bi [UNK] [UNK] ma kana [UNK] [UNK] [SEP]'}, {'score': 0.03714632987976074, 'token': 11279, 'token_str': 'ai', 'sequence': '[CLS] ai [UNK] paoni ka [UNK] [UNK] ho koa [UNK] [MASK] bi [UNK] [UNK] ma kana [UNK] [UNK] [SEP]'}, {'score': 0.03418014943599701, 'token': 143, 'token_str': 'a', 'sequence': '[CLS] a [UNK] paoni ka [UNK] [UNK] ho koa [UNK] [MASK] bi [UNK] [UNK] ma kana [UNK] [UNK] [SEP]'}, {'score': 0.03270101174712181, 'token': 10900, 'token_str': 'ki', 'sequence': '[CLS] ki [UNK] paoni ka [UNK] [UNK] ho koa [UNK] [MASK] bi [UNK] [UNK] ma kana [UNK] [UNK] [SEP]'}]\n",
      "Unexpected format for prediction at index 1: [{'score': 0.09395667910575867, 'token': 10507, 'token_str': 'ma', 'sequence': '[CLS] [MASK] [UNK] paoni ka [UNK] [UNK] ho koa [UNK] ma bi [UNK] [UNK] ma kana [UNK] [UNK] [SEP]'}, {'score': 0.03281894326210022, 'token': 10346, 'token_str': 'be', 'sequence': '[CLS] [MASK] [UNK] paoni ka [UNK] [UNK] ho koa [UNK] be bi [UNK] [UNK] ma kana [UNK] [UNK] [SEP]'}, {'score': 0.032563358545303345, 'token': 11117, 'token_str': 'bo', 'sequence': '[CLS] [MASK] [UNK] paoni ka [UNK] [UNK] ho koa [UNK] bo bi [UNK] [UNK] ma kana [UNK] [UNK] [SEP]'}, {'score': 0.029708005487918854, 'token': 10616, 'token_str': 'ho', 'sequence': '[CLS] [MASK] [UNK] paoni ka [UNK] [UNK] ho koa [UNK] ho bi [UNK] [UNK] ma kana [UNK] [UNK] [SEP]'}, {'score': 0.025456301867961884, 'token': 11742, 'token_str': 'ra', 'sequence': '[CLS] [MASK] [UNK] paoni ka [UNK] [UNK] ho koa [UNK] ra bi [UNK] [UNK] ma kana [UNK] [UNK] [SEP]'}]\n",
      "Unexpected format for prediction at index 0: [{'score': 0.04857339337468147, 'token': 143, 'token_str': 'a', 'sequence': '[CLS] [UNK] biribi [UNK] [UNK] [UNK] [UNK] a [MASK] in kin tana [UNK] ribi [UNK] kasi [SEP]'}, {'score': 0.03325185552239418, 'token': 161, 'token_str': 's', 'sequence': '[CLS] [UNK] biribi [UNK] [UNK] [UNK] [UNK] s [MASK] in kin tana [UNK] ribi [UNK] kasi [SEP]'}, {'score': 0.02759586088359356, 'token': 10237, 'token_str': 'ka', 'sequence': '[CLS] [UNK] biribi [UNK] [UNK] [UNK] [UNK] ka [MASK] in kin tana [UNK] ribi [UNK] kasi [SEP]'}, {'score': 0.02442042902112007, 'token': 151, 'token_str': 'i', 'sequence': '[CLS] [UNK] biribi [UNK] [UNK] [UNK] [UNK] i [MASK] in kin tana [UNK] ribi [UNK] kasi [SEP]'}, {'score': 0.023702826350927353, 'token': 10863, 'token_str': 'bi', 'sequence': '[CLS] [UNK] biribi [UNK] [UNK] [UNK] [UNK] bi [MASK] in kin tana [UNK] ribi [UNK] kasi [SEP]'}]\n",
      "Unexpected format for prediction at index 1: [{'score': 0.06734807044267654, 'token': 10116, 'token_str': '##i', 'sequence': '[CLS] [UNK] biribi [UNK] [UNK] [UNK] [UNK] [MASK]i in kin tana [UNK] ribi [UNK] kasi [SEP]'}, {'score': 0.04679876193404198, 'token': 161, 'token_str': 's', 'sequence': '[CLS] [UNK] biribi [UNK] [UNK] [UNK] [UNK] [MASK] s in kin tana [UNK] ribi [UNK] kasi [SEP]'}, {'score': 0.04138273373246193, 'token': 143, 'token_str': 'a', 'sequence': '[CLS] [UNK] biribi [UNK] [UNK] [UNK] [UNK] [MASK] a in kin tana [UNK] ribi [UNK] kasi [SEP]'}, {'score': 0.03622964397072792, 'token': 10115, 'token_str': '##n', 'sequence': '[CLS] [UNK] biribi [UNK] [UNK] [UNK] [UNK] [MASK]n in kin tana [UNK] ribi [UNK] kasi [SEP]'}, {'score': 0.0295871514827013, 'token': 10112, 'token_str': '##a', 'sequence': '[CLS] [UNK] biribi [UNK] [UNK] [UNK] [UNK] [MASK]a in kin tana [UNK] ribi [UNK] kasi [SEP]'}]\n",
      "Unexpected format for prediction at index 0: [{'score': 0.06742775440216064, 'token': 82714, 'token_str': 'mea', 'sequence': '[CLS] [UNK] [UNK] mea [UNK] [UNK] [MASK] [UNK] [UNK] [UNK] [UNK] [UNK] ya mea [UNK] [UNK] ya mea [MASK] [UNK] [UNK] [UNK] [SEP]'}, {'score': 0.062346115708351135, 'token': 10237, 'token_str': 'ka', 'sequence': '[CLS] [UNK] [UNK] ka [UNK] [UNK] [MASK] [UNK] [UNK] [UNK] [UNK] [UNK] ya mea [UNK] [UNK] ya mea [MASK] [UNK] [UNK] [UNK] [SEP]'}, {'score': 0.03722350671887398, 'token': 10900, 'token_str': 'ki', 'sequence': '[CLS] [UNK] [UNK] ki [UNK] [UNK] [MASK] [UNK] [UNK] [UNK] [UNK] [UNK] ya mea [UNK] [UNK] ya mea [MASK] [UNK] [UNK] [UNK] [SEP]'}, {'score': 0.03602562099695206, 'token': 143, 'token_str': 'a', 'sequence': '[CLS] [UNK] [UNK] a [UNK] [UNK] [MASK] [UNK] [UNK] [UNK] [UNK] [UNK] ya mea [UNK] [UNK] ya mea [MASK] [UNK] [UNK] [UNK] [SEP]'}, {'score': 0.0300558153539896, 'token': 11009, 'token_str': 'ke', 'sequence': '[CLS] [UNK] [UNK] ke [UNK] [UNK] [MASK] [UNK] [UNK] [UNK] [UNK] [UNK] ya mea [UNK] [UNK] ya mea [MASK] [UNK] [UNK] [UNK] [SEP]'}]\n",
      "Unexpected format for prediction at index 1: [{'score': 0.052500925958156586, 'token': 10237, 'token_str': 'ka', 'sequence': '[CLS] [UNK] [UNK] [MASK] [UNK] [UNK] ka [UNK] [UNK] [UNK] [UNK] [UNK] ya mea [UNK] [UNK] ya mea [MASK] [UNK] [UNK] [UNK] [SEP]'}, {'score': 0.043370574712753296, 'token': 82714, 'token_str': 'mea', 'sequence': '[CLS] [UNK] [UNK] [MASK] [UNK] [UNK] mea [UNK] [UNK] [UNK] [UNK] [UNK] ya mea [UNK] [UNK] ya mea [MASK] [UNK] [UNK] [UNK] [SEP]'}, {'score': 0.034994009882211685, 'token': 11279, 'token_str': 'ai', 'sequence': '[CLS] [UNK] [UNK] [MASK] [UNK] [UNK] ai [UNK] [UNK] [UNK] [UNK] [UNK] ya mea [UNK] [UNK] ya mea [MASK] [UNK] [UNK] [UNK] [SEP]'}, {'score': 0.032268594950437546, 'token': 10863, 'token_str': 'bi', 'sequence': '[CLS] [UNK] [UNK] [MASK] [UNK] [UNK] bi [UNK] [UNK] [UNK] [UNK] [UNK] ya mea [UNK] [UNK] ya mea [MASK] [UNK] [UNK] [UNK] [SEP]'}, {'score': 0.03203829377889633, 'token': 10900, 'token_str': 'ki', 'sequence': '[CLS] [UNK] [UNK] [MASK] [UNK] [UNK] ki [UNK] [UNK] [UNK] [UNK] [UNK] ya mea [UNK] [UNK] ya mea [MASK] [UNK] [UNK] [UNK] [SEP]'}]\n",
      "Unexpected format for prediction at index 2: [{'score': 0.17838844656944275, 'token': 10115, 'token_str': '##n', 'sequence': '[CLS] [UNK] [UNK] [MASK] [UNK] [UNK] [MASK] [UNK] [UNK] [UNK] [UNK] [UNK] ya mea [UNK] [UNK] ya mean [UNK] [UNK] [UNK] [SEP]'}, {'score': 0.08964016288518906, 'token': 10116, 'token_str': '##i', 'sequence': '[CLS] [UNK] [UNK] [MASK] [UNK] [UNK] [MASK] [UNK] [UNK] [UNK] [UNK] [UNK] ya mea [UNK] [UNK] ya meai [UNK] [UNK] [UNK] [SEP]'}, {'score': 0.05551338195800781, 'token': 11009, 'token_str': 'ke', 'sequence': '[CLS] [UNK] [UNK] [MASK] [UNK] [UNK] [MASK] [UNK] [UNK] [UNK] [UNK] [UNK] ya mea [UNK] [UNK] ya mea ke [UNK] [UNK] [UNK] [SEP]'}, {'score': 0.05495843663811684, 'token': 151, 'token_str': 'i', 'sequence': '[CLS] [UNK] [UNK] [MASK] [UNK] [UNK] [MASK] [UNK] [UNK] [UNK] [UNK] [UNK] ya mea [UNK] [UNK] ya mea i [UNK] [UNK] [UNK] [SEP]'}, {'score': 0.03514295071363449, 'token': 143, 'token_str': 'a', 'sequence': '[CLS] [UNK] [UNK] [MASK] [UNK] [UNK] [MASK] [UNK] [UNK] [UNK] [UNK] [UNK] ya mea [UNK] [UNK] ya mea a [UNK] [UNK] [UNK] [SEP]'}]\n",
      "Unexpected format for prediction at index 0: [{'score': 0.05928465723991394, 'token': 10237, 'token_str': 'ka', 'sequence': '[CLS] [UNK] mis ha [UNK] ka [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] x oa [UNK] [MASK] [UNK] ke [SEP]'}, {'score': 0.04149317741394043, 'token': 143, 'token_str': 'a', 'sequence': '[CLS] [UNK] mis ha [UNK] a [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] x oa [UNK] [MASK] [UNK] ke [SEP]'}, {'score': 0.04013291001319885, 'token': 11742, 'token_str': 'ra', 'sequence': '[CLS] [UNK] mis ha [UNK] ra [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] x oa [UNK] [MASK] [UNK] ke [SEP]'}, {'score': 0.038755737245082855, 'token': 10900, 'token_str': 'ki', 'sequence': '[CLS] [UNK] mis ha [UNK] ki [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] x oa [UNK] [MASK] [UNK] ke [SEP]'}, {'score': 0.027230776846408844, 'token': 11279, 'token_str': 'ai', 'sequence': '[CLS] [UNK] mis ha [UNK] ai [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] x oa [UNK] [MASK] [UNK] ke [SEP]'}]\n",
      "Unexpected format for prediction at index 1: [{'score': 0.0897994339466095, 'token': 10237, 'token_str': 'ka', 'sequence': '[CLS] [UNK] mis ha [UNK] [MASK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] x oa [UNK] ka [UNK] ke [SEP]'}, {'score': 0.04972955957055092, 'token': 11009, 'token_str': 'ke', 'sequence': '[CLS] [UNK] mis ha [UNK] [MASK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] x oa [UNK] ke [UNK] ke [SEP]'}, {'score': 0.039111021906137466, 'token': 143, 'token_str': 'a', 'sequence': '[CLS] [UNK] mis ha [UNK] [MASK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] x oa [UNK] a [UNK] ke [SEP]'}, {'score': 0.031259797513484955, 'token': 11742, 'token_str': 'ra', 'sequence': '[CLS] [UNK] mis ha [UNK] [MASK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] x oa [UNK] ra [UNK] ke [SEP]'}, {'score': 0.03089873306453228, 'token': 10900, 'token_str': 'ki', 'sequence': '[CLS] [UNK] mis ha [UNK] [MASK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] x oa [UNK] ki [UNK] ke [SEP]'}]\n",
      "Unexpected format for prediction at index 0: [{'score': 0.046713463962078094, 'token': 143, 'token_str': 'a', 'sequence': '[CLS] [UNK] [UNK] [UNK] tian [UNK] [UNK] a to r c en in [UNK] [UNK] ke [MASK] qui e ra [MASK] u di an kin [SEP]'}, {'score': 0.042512837797403336, 'token': 160, 'token_str': 'r', 'sequence': '[CLS] [UNK] [UNK] [UNK] tian [UNK] [UNK] r to r c en in [UNK] [UNK] ke [MASK] qui e ra [MASK] u di an kin [SEP]'}, {'score': 0.04098397493362427, 'token': 151, 'token_str': 'i', 'sequence': '[CLS] [UNK] [UNK] [UNK] tian [UNK] [UNK] i to r c en in [UNK] [UNK] ke [MASK] qui e ra [MASK] u di an kin [SEP]'}, {'score': 0.03878592327237129, 'token': 154, 'token_str': 'l', 'sequence': '[CLS] [UNK] [UNK] [UNK] tian [UNK] [UNK] l to r c en in [UNK] [UNK] ke [MASK] qui e ra [MASK] u di an kin [SEP]'}, {'score': 0.031544674187898636, 'token': 157, 'token_str': 'o', 'sequence': '[CLS] [UNK] [UNK] [UNK] tian [UNK] [UNK] o to r c en in [UNK] [UNK] ke [MASK] qui e ra [MASK] u di an kin [SEP]'}]\n",
      "Unexpected format for prediction at index 1: [{'score': 0.05732385441660881, 'token': 161, 'token_str': 's', 'sequence': '[CLS] [UNK] [UNK] [UNK] tian [UNK] [UNK] [MASK] to r c en in [UNK] [UNK] ke s qui e ra [MASK] u di an kin [SEP]'}, {'score': 0.0454537570476532, 'token': 157, 'token_str': 'o', 'sequence': '[CLS] [UNK] [UNK] [UNK] tian [UNK] [UNK] [MASK] to r c en in [UNK] [UNK] ke o qui e ra [MASK] u di an kin [SEP]'}, {'score': 0.04260771721601486, 'token': 143, 'token_str': 'a', 'sequence': '[CLS] [UNK] [UNK] [UNK] tian [UNK] [UNK] [MASK] to r c en in [UNK] [UNK] ke a qui e ra [MASK] u di an kin [SEP]'}, {'score': 0.04202679172158241, 'token': 154, 'token_str': 'l', 'sequence': '[CLS] [UNK] [UNK] [UNK] tian [UNK] [UNK] [MASK] to r c en in [UNK] [UNK] ke l qui e ra [MASK] u di an kin [SEP]'}, {'score': 0.03219561278820038, 'token': 151, 'token_str': 'i', 'sequence': '[CLS] [UNK] [UNK] [UNK] tian [UNK] [UNK] [MASK] to r c en in [UNK] [UNK] ke i qui e ra [MASK] u di an kin [SEP]'}]\n",
      "Unexpected format for prediction at index 2: [{'score': 0.06690340489149094, 'token': 10154, 'token_str': 'do', 'sequence': '[CLS] [UNK] [UNK] [UNK] tian [UNK] [UNK] [MASK] to r c en in [UNK] [UNK] ke [MASK] qui e ra do u di an kin [SEP]'}, {'score': 0.06443294137716293, 'token': 151, 'token_str': 'i', 'sequence': '[CLS] [UNK] [UNK] [UNK] tian [UNK] [UNK] [MASK] to r c en in [UNK] [UNK] ke [MASK] qui e ra i u di an kin [SEP]'}, {'score': 0.06264939904212952, 'token': 157, 'token_str': 'o', 'sequence': '[CLS] [UNK] [UNK] [UNK] tian [UNK] [UNK] [MASK] to r c en in [UNK] [UNK] ke [MASK] qui e ra o u di an kin [SEP]'}, {'score': 0.04370168223977089, 'token': 161, 'token_str': 's', 'sequence': '[CLS] [UNK] [UNK] [UNK] tian [UNK] [UNK] [MASK] to r c en in [UNK] [UNK] ke [MASK] qui e ra s u di an kin [SEP]'}, {'score': 0.04127418249845505, 'token': 143, 'token_str': 'a', 'sequence': '[CLS] [UNK] [UNK] [UNK] tian [UNK] [UNK] [MASK] to r c en in [UNK] [UNK] ke [MASK] qui e ra a u di an kin [SEP]'}]\n",
      "Unexpected format for prediction at index 0: [{'score': 0.08780542761087418, 'token': 10237, 'token_str': 'ka', 'sequence': '[CLS] [UNK] [UNK] kana [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] ka ribi [UNK] [UNK] ka [UNK] [MASK] [UNK] [SEP]'}, {'score': 0.06406710296869278, 'token': 143, 'token_str': 'a', 'sequence': '[CLS] [UNK] [UNK] kana [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] a ribi [UNK] [UNK] ka [UNK] [MASK] [UNK] [SEP]'}, {'score': 0.03161896765232086, 'token': 151, 'token_str': 'i', 'sequence': '[CLS] [UNK] [UNK] kana [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] i ribi [UNK] [UNK] ka [UNK] [MASK] [UNK] [SEP]'}, {'score': 0.02914925292134285, 'token': 11009, 'token_str': 'ke', 'sequence': '[CLS] [UNK] [UNK] kana [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] ke ribi [UNK] [UNK] ka [UNK] [MASK] [UNK] [SEP]'}, {'score': 0.022737188264727592, 'token': 10900, 'token_str': 'ki', 'sequence': '[CLS] [UNK] [UNK] kana [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] ki ribi [UNK] [UNK] ka [UNK] [MASK] [UNK] [SEP]'}]\n",
      "Unexpected format for prediction at index 1: [{'score': 0.17511990666389465, 'token': 10237, 'token_str': 'ka', 'sequence': '[CLS] [UNK] [UNK] kana [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [MASK] ribi [UNK] [UNK] ka [UNK] ka [UNK] [SEP]'}, {'score': 0.03450019285082817, 'token': 11009, 'token_str': 'ke', 'sequence': '[CLS] [UNK] [UNK] kana [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [MASK] ribi [UNK] [UNK] ka [UNK] ke [UNK] [SEP]'}, {'score': 0.03291356563568115, 'token': 143, 'token_str': 'a', 'sequence': '[CLS] [UNK] [UNK] kana [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [MASK] ribi [UNK] [UNK] ka [UNK] a [UNK] [SEP]'}, {'score': 0.029750868678092957, 'token': 10900, 'token_str': 'ki', 'sequence': '[CLS] [UNK] [UNK] kana [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [MASK] ribi [UNK] [UNK] ka [UNK] ki [UNK] [SEP]'}, {'score': 0.026695072650909424, 'token': 10863, 'token_str': 'bi', 'sequence': '[CLS] [UNK] [UNK] kana [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [MASK] ribi [UNK] [UNK] ka [UNK] bi [UNK] [SEP]'}]\n",
      "Unexpected format for prediction at index 0: [{'score': 0.057255394756793976, 'token': 10237, 'token_str': 'ka', 'sequence': '[CLS] [UNK] man chon chon man [UNK] [UNK] [UNK] [UNK] [UNK] ka [UNK] [MASK] [UNK] [UNK] ribi [SEP]'}, {'score': 0.039798855781555176, 'token': 11009, 'token_str': 'ke', 'sequence': '[CLS] [UNK] man chon chon man [UNK] [UNK] [UNK] [UNK] [UNK] ke [UNK] [MASK] [UNK] [UNK] ribi [SEP]'}, {'score': 0.0396849662065506, 'token': 143, 'token_str': 'a', 'sequence': '[CLS] [UNK] man chon chon man [UNK] [UNK] [UNK] [UNK] [UNK] a [UNK] [MASK] [UNK] [UNK] ribi [SEP]'}, {'score': 0.03714245557785034, 'token': 10863, 'token_str': 'bi', 'sequence': '[CLS] [UNK] man chon chon man [UNK] [UNK] [UNK] [UNK] [UNK] bi [UNK] [MASK] [UNK] [UNK] ribi [SEP]'}, {'score': 0.03324608877301216, 'token': 10900, 'token_str': 'ki', 'sequence': '[CLS] [UNK] man chon chon man [UNK] [UNK] [UNK] [UNK] [UNK] ki [UNK] [MASK] [UNK] [UNK] ribi [SEP]'}]\n",
      "Unexpected format for prediction at index 1: [{'score': 0.0455818697810173, 'token': 10237, 'token_str': 'ka', 'sequence': '[CLS] [UNK] man chon chon man [UNK] [UNK] [UNK] [UNK] [UNK] [MASK] [UNK] ka [UNK] [UNK] ribi [SEP]'}, {'score': 0.04085453227162361, 'token': 143, 'token_str': 'a', 'sequence': '[CLS] [UNK] man chon chon man [UNK] [UNK] [UNK] [UNK] [UNK] [MASK] [UNK] a [UNK] [UNK] ribi [SEP]'}, {'score': 0.039935242384672165, 'token': 10863, 'token_str': 'bi', 'sequence': '[CLS] [UNK] man chon chon man [UNK] [UNK] [UNK] [UNK] [UNK] [MASK] [UNK] bi [UNK] [UNK] ribi [SEP]'}, {'score': 0.033029649406671524, 'token': 11009, 'token_str': 'ke', 'sequence': '[CLS] [UNK] man chon chon man [UNK] [UNK] [UNK] [UNK] [UNK] [MASK] [UNK] ke [UNK] [UNK] ribi [SEP]'}, {'score': 0.029048452153801918, 'token': 10900, 'token_str': 'ki', 'sequence': '[CLS] [UNK] man chon chon man [UNK] [UNK] [UNK] [UNK] [UNK] [MASK] [UNK] ki [UNK] [UNK] ribi [SEP]'}]\n",
      "Unexpected format for prediction at index 0: [{'score': 0.05393216013908386, 'token': 51252, 'token_str': 'sha', 'sequence': '[CLS] [UNK] sha [MASK] ba ina [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] sha [UNK] ha koa [SEP]'}, {'score': 0.04776047170162201, 'token': 10593, 'token_str': 'ya', 'sequence': '[CLS] [UNK] ya [MASK] ba ina [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] sha [UNK] ha koa [SEP]'}, {'score': 0.046316735446453094, 'token': 63135, 'token_str': 'yama', 'sequence': '[CLS] [UNK] yama [MASK] ba ina [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] sha [UNK] ha koa [SEP]'}, {'score': 0.03789301961660385, 'token': 10237, 'token_str': 'ka', 'sequence': '[CLS] [UNK] ka [MASK] ba ina [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] sha [UNK] ha koa [SEP]'}, {'score': 0.031514350324869156, 'token': 10863, 'token_str': 'bi', 'sequence': '[CLS] [UNK] bi [MASK] ba ina [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] sha [UNK] ha koa [SEP]'}]\n",
      "Unexpected format for prediction at index 1: [{'score': 0.045853391289711, 'token': 10116, 'token_str': '##i', 'sequence': '[CLS] [UNK] [MASK]i ba ina [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] sha [UNK] ha koa [SEP]'}, {'score': 0.04094194248318672, 'token': 143, 'token_str': 'a', 'sequence': '[CLS] [UNK] [MASK] a ba ina [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] sha [UNK] ha koa [SEP]'}, {'score': 0.037459030747413635, 'token': 10115, 'token_str': '##n', 'sequence': '[CLS] [UNK] [MASK]n ba ina [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] sha [UNK] ha koa [SEP]'}, {'score': 0.03126242011785507, 'token': 10112, 'token_str': '##a', 'sequence': '[CLS] [UNK] [MASK]a ba ina [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] sha [UNK] ha koa [SEP]'}, {'score': 0.02921166643500328, 'token': 151, 'token_str': 'i', 'sequence': '[CLS] [UNK] [MASK] i ba ina [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] sha [UNK] ha koa [SEP]'}]\n",
      "Unexpected format for prediction at index 0: [{'score': 0.11213719844818115, 'token': 11742, 'token_str': 'ra', 'sequence': '[CLS] [UNK] [UNK] ra [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] r g aka [MASK] g [MASK] [UNK] ra [UNK] ra be kin [SEP]'}, {'score': 0.051734428852796555, 'token': 10237, 'token_str': 'ka', 'sequence': '[CLS] [UNK] [UNK] ka [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] r g aka [MASK] g [MASK] [UNK] ra [UNK] ra be kin [SEP]'}, {'score': 0.038403548300266266, 'token': 10507, 'token_str': 'ma', 'sequence': '[CLS] [UNK] [UNK] ma [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] r g aka [MASK] g [MASK] [UNK] ra [UNK] ra be kin [SEP]'}, {'score': 0.03521319478750229, 'token': 143, 'token_str': 'a', 'sequence': '[CLS] [UNK] [UNK] a [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] r g aka [MASK] g [MASK] [UNK] ra [UNK] ra be kin [SEP]'}, {'score': 0.028728757053613663, 'token': 19716, 'token_str': 'ri', 'sequence': '[CLS] [UNK] [UNK] ri [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] r g aka [MASK] g [MASK] [UNK] ra [UNK] ra be kin [SEP]'}]\n",
      "Unexpected format for prediction at index 1: [{'score': 0.9425499439239502, 'token': 160, 'token_str': 'r', 'sequence': '[CLS] [UNK] [UNK] [MASK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] r g aka r g [MASK] [UNK] ra [UNK] ra be kin [SEP]'}, {'score': 0.006979383993893862, 'token': 10115, 'token_str': '##n', 'sequence': '[CLS] [UNK] [UNK] [MASK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] r g akan g [MASK] [UNK] ra [UNK] ra be kin [SEP]'}, {'score': 0.003675710177049041, 'token': 11742, 'token_str': 'ra', 'sequence': '[CLS] [UNK] [UNK] [MASK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] r g aka ra g [MASK] [UNK] ra [UNK] ra be kin [SEP]'}, {'score': 0.0033518413547426462, 'token': 19716, 'token_str': 'ri', 'sequence': '[CLS] [UNK] [UNK] [MASK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] r g aka ri g [MASK] [UNK] ra [UNK] ra be kin [SEP]'}, {'score': 0.0025699015241116285, 'token': 151, 'token_str': 'i', 'sequence': '[CLS] [UNK] [UNK] [MASK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] r g aka i g [MASK] [UNK] ra [UNK] ra be kin [SEP]'}]\n",
      "Unexpected format for prediction at index 2: [{'score': 0.9995137453079224, 'token': 36710, 'token_str': 'aka', 'sequence': '[CLS] [UNK] [UNK] [MASK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] r g aka [MASK] g aka [UNK] ra [UNK] ra be kin [SEP]'}, {'score': 8.803868695395067e-05, 'token': 61370, 'token_str': 'tima', 'sequence': '[CLS] [UNK] [UNK] [MASK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] r g aka [MASK] g tima [UNK] ra [UNK] ra be kin [SEP]'}, {'score': 4.1124698327621445e-05, 'token': 22079, 'token_str': 'ama', 'sequence': '[CLS] [UNK] [UNK] [MASK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] r g aka [MASK] g ama [UNK] ra [UNK] ra be kin [SEP]'}, {'score': 3.713850674103014e-05, 'token': 10593, 'token_str': 'ya', 'sequence': '[CLS] [UNK] [UNK] [MASK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] r g aka [MASK] g ya [UNK] ra [UNK] ra be kin [SEP]'}, {'score': 3.598197508836165e-05, 'token': 10115, 'token_str': '##n', 'sequence': '[CLS] [UNK] [UNK] [MASK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] r g aka [MASK] gn [UNK] ra [UNK] ra be kin [SEP]'}]\n",
      "Unexpected format for prediction at index 0: [{'score': 0.10330686718225479, 'token': 10237, 'token_str': 'ka', 'sequence': '[CLS] [UNK] [UNK] ra [UNK] ya ka [UNK] [UNK] ka ya [UNK] [MASK] bei betin [SEP]'}, {'score': 0.0763729065656662, 'token': 10116, 'token_str': '##i', 'sequence': '[CLS] [UNK] [UNK] ra [UNK] yai [UNK] [UNK] ka ya [UNK] [MASK] bei betin [SEP]'}, {'score': 0.07531598955392838, 'token': 11009, 'token_str': 'ke', 'sequence': '[CLS] [UNK] [UNK] ra [UNK] ya ke [UNK] [UNK] ka ya [UNK] [MASK] bei betin [SEP]'}, {'score': 0.04251360148191452, 'token': 151, 'token_str': 'i', 'sequence': '[CLS] [UNK] [UNK] ra [UNK] ya i [UNK] [UNK] ka ya [UNK] [MASK] bei betin [SEP]'}, {'score': 0.03569316491484642, 'token': 11742, 'token_str': 'ra', 'sequence': '[CLS] [UNK] [UNK] ra [UNK] ya ra [UNK] [UNK] ka ya [UNK] [MASK] bei betin [SEP]'}]\n",
      "Unexpected format for prediction at index 1: [{'score': 0.0451485700905323, 'token': 143, 'token_str': 'a', 'sequence': '[CLS] [UNK] [UNK] ra [UNK] ya [MASK] [UNK] [UNK] ka ya [UNK] a bei betin [SEP]'}, {'score': 0.03870762139558792, 'token': 10237, 'token_str': 'ka', 'sequence': '[CLS] [UNK] [UNK] ra [UNK] ya [MASK] [UNK] [UNK] ka ya [UNK] ka bei betin [SEP]'}, {'score': 0.033755917102098465, 'token': 10863, 'token_str': 'bi', 'sequence': '[CLS] [UNK] [UNK] ra [UNK] ya [MASK] [UNK] [UNK] ka ya [UNK] bi bei betin [SEP]'}, {'score': 0.027460407465696335, 'token': 11009, 'token_str': 'ke', 'sequence': '[CLS] [UNK] [UNK] ra [UNK] ya [MASK] [UNK] [UNK] ka ya [UNK] ke bei betin [SEP]'}, {'score': 0.02496730349957943, 'token': 11117, 'token_str': 'bo', 'sequence': '[CLS] [UNK] [UNK] ra [UNK] ya [MASK] [UNK] [UNK] ka ya [UNK] bo bei betin [SEP]'}]\n",
      "Unexpected format for prediction at index 0: [{'score': 0.10840415209531784, 'token': 151, 'token_str': 'i', 'sequence': '[CLS] [UNK] [UNK] osi [UNK] [UNK] shama i [UNK] [UNK] i [MASK] [UNK] ka [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [SEP]'}, {'score': 0.05937633290886879, 'token': 11009, 'token_str': 'ke', 'sequence': '[CLS] [UNK] [UNK] osi [UNK] [UNK] shama ke [UNK] [UNK] i [MASK] [UNK] ka [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [SEP]'}, {'score': 0.05602894350886345, 'token': 10237, 'token_str': 'ka', 'sequence': '[CLS] [UNK] [UNK] osi [UNK] [UNK] shama ka [UNK] [UNK] i [MASK] [UNK] ka [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [SEP]'}, {'score': 0.05389317125082016, 'token': 10900, 'token_str': 'ki', 'sequence': '[CLS] [UNK] [UNK] osi [UNK] [UNK] shama ki [UNK] [UNK] i [MASK] [UNK] ka [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [SEP]'}, {'score': 0.053382162004709244, 'token': 143, 'token_str': 'a', 'sequence': '[CLS] [UNK] [UNK] osi [UNK] [UNK] shama a [UNK] [UNK] i [MASK] [UNK] ka [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [SEP]'}]\n",
      "Unexpected format for prediction at index 1: [{'score': 0.12171497941017151, 'token': 10237, 'token_str': 'ka', 'sequence': '[CLS] [UNK] [UNK] osi [UNK] [UNK] shama [MASK] [UNK] [UNK] i ka [UNK] ka [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [SEP]'}, {'score': 0.05279859900474548, 'token': 143, 'token_str': 'a', 'sequence': '[CLS] [UNK] [UNK] osi [UNK] [UNK] shama [MASK] [UNK] [UNK] i a [UNK] ka [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [SEP]'}, {'score': 0.03823583573102951, 'token': 151, 'token_str': 'i', 'sequence': '[CLS] [UNK] [UNK] osi [UNK] [UNK] shama [MASK] [UNK] [UNK] i i [UNK] ka [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [SEP]'}, {'score': 0.032724201679229736, 'token': 11009, 'token_str': 'ke', 'sequence': '[CLS] [UNK] [UNK] osi [UNK] [UNK] shama [MASK] [UNK] [UNK] i ke [UNK] ka [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [SEP]'}, {'score': 0.0317080095410347, 'token': 10900, 'token_str': 'ki', 'sequence': '[CLS] [UNK] [UNK] osi [UNK] [UNK] shama [MASK] [UNK] [UNK] i ki [UNK] ka [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [SEP]'}]\n",
      "Unexpected format for prediction at index 0: [{'score': 0.5544123649597168, 'token': 157, 'token_str': 'o', 'sequence': '[CLS] [UNK] ris to bo [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] ka [UNK] o hati [UNK] o hati [UNK] on [UNK] si [UNK] on [MASK] si [MASK] [SEP]'}, {'score': 0.02986866794526577, 'token': 10104, 'token_str': 'in', 'sequence': '[CLS] [UNK] ris to bo [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] ka [UNK] in hati [UNK] o hati [UNK] on [UNK] si [UNK] on [MASK] si [MASK] [SEP]'}, {'score': 0.024249810725450516, 'token': 143, 'token_str': 'a', 'sequence': '[CLS] [UNK] ris to bo [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] ka [UNK] a hati [UNK] o hati [UNK] on [UNK] si [UNK] on [MASK] si [MASK] [SEP]'}, {'score': 0.017275987192988396, 'token': 147, 'token_str': 'e', 'sequence': '[CLS] [UNK] ris to bo [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] ka [UNK] e hati [UNK] o hati [UNK] on [UNK] si [UNK] on [MASK] si [MASK] [SEP]'}, {'score': 0.01554683968424797, 'token': 14114, 'token_str': 'ua', 'sequence': '[CLS] [UNK] ris to bo [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] ka [UNK] ua hati [UNK] o hati [UNK] on [UNK] si [UNK] on [MASK] si [MASK] [SEP]'}]\n",
      "Unexpected format for prediction at index 1: [{'score': 0.22492489218711853, 'token': 157, 'token_str': 'o', 'sequence': '[CLS] [UNK] ris to bo [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] ka [UNK] [MASK] hati [UNK] o hati [UNK] on [UNK] si [UNK] on o si [MASK] [SEP]'}, {'score': 0.07692152261734009, 'token': 151, 'token_str': 'i', 'sequence': '[CLS] [UNK] ris to bo [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] ka [UNK] [MASK] hati [UNK] o hati [UNK] on [UNK] si [UNK] on i si [MASK] [SEP]'}, {'score': 0.06574904918670654, 'token': 143, 'token_str': 'a', 'sequence': '[CLS] [UNK] ris to bo [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] ka [UNK] [MASK] hati [UNK] o hati [UNK] on [UNK] si [UNK] on a si [MASK] [SEP]'}, {'score': 0.03962356969714165, 'token': 10115, 'token_str': '##n', 'sequence': '[CLS] [UNK] ris to bo [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] ka [UNK] [MASK] hati [UNK] o hati [UNK] on [UNK] si [UNK] onn si [MASK] [SEP]'}, {'score': 0.03937814384698868, 'token': 10457, 'token_str': '##ko', 'sequence': '[CLS] [UNK] ris to bo [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] ka [UNK] [MASK] hati [UNK] o hati [UNK] on [UNK] si [UNK] onko si [MASK] [SEP]'}]\n",
      "Unexpected format for prediction at index 2: [{'score': 0.9622120261192322, 'token': 43414, 'token_str': 'hati', 'sequence': '[CLS] [UNK] ris to bo [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] ka [UNK] [MASK] hati [UNK] o hati [UNK] on [UNK] si [UNK] on [MASK] si hati [SEP]'}, {'score': 0.008305891416966915, 'token': 11880, 'token_str': '##bi', 'sequence': '[CLS] [UNK] ris to bo [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] ka [UNK] [MASK] hati [UNK] o hati [UNK] on [UNK] si [UNK] on [MASK] sibi [SEP]'}, {'score': 0.007948842830955982, 'token': 11117, 'token_str': 'bo', 'sequence': '[CLS] [UNK] ris to bo [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] ka [UNK] [MASK] hati [UNK] o hati [UNK] on [UNK] si [UNK] on [MASK] si bo [SEP]'}, {'score': 0.0013708662008866668, 'token': 21854, 'token_str': 'pari', 'sequence': '[CLS] [UNK] ris to bo [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] ka [UNK] [MASK] hati [UNK] o hati [UNK] on [UNK] si [UNK] on [MASK] si pari [SEP]'}, {'score': 0.0013509057462215424, 'token': 10346, 'token_str': 'be', 'sequence': '[CLS] [UNK] ris to bo [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] ka [UNK] [MASK] hati [UNK] o hati [UNK] on [UNK] si [UNK] on [MASK] si be [SEP]'}]\n",
      "Unexpected format for prediction at index 0: [{'score': 0.999976396560669, 'token': 154, 'token_str': 'l', 'sequence': '[CLS] [UNK] [UNK] x p l ic o [UNK] [UNK] [MASK] x p l ic o [UNK] [UNK] [SEP]'}, {'score': 9.682489690021612e-06, 'token': 161, 'token_str': 's', 'sequence': '[CLS] [UNK] [UNK] x p s ic o [UNK] [UNK] [MASK] x p l ic o [UNK] [UNK] [SEP]'}, {'score': 4.1761190914257895e-06, 'token': 156, 'token_str': 'n', 'sequence': '[CLS] [UNK] [UNK] x p n ic o [UNK] [UNK] [MASK] x p l ic o [UNK] [UNK] [SEP]'}, {'score': 3.303582161606755e-06, 'token': 160, 'token_str': 'r', 'sequence': '[CLS] [UNK] [UNK] x p r ic o [UNK] [UNK] [MASK] x p l ic o [UNK] [UNK] [SEP]'}, {'score': 1.8164738548875903e-06, 'token': 155, 'token_str': 'm', 'sequence': '[CLS] [UNK] [UNK] x p m ic o [UNK] [UNK] [MASK] x p l ic o [UNK] [UNK] [SEP]'}]\n",
      "Unexpected format for prediction at index 1: [{'score': 0.04780297353863716, 'token': 143, 'token_str': 'a', 'sequence': '[CLS] [UNK] [UNK] x p [MASK] ic o [UNK] [UNK] a x p l ic o [UNK] [UNK] [SEP]'}, {'score': 0.04726380854845047, 'token': 151, 'token_str': 'i', 'sequence': '[CLS] [UNK] [UNK] x p [MASK] ic o [UNK] [UNK] i x p l ic o [UNK] [UNK] [SEP]'}, {'score': 0.034054096788167953, 'token': 161, 'token_str': 's', 'sequence': '[CLS] [UNK] [UNK] x p [MASK] ic o [UNK] [UNK] s x p l ic o [UNK] [UNK] [SEP]'}, {'score': 0.02958567999303341, 'token': 160, 'token_str': 'r', 'sequence': '[CLS] [UNK] [UNK] x p [MASK] ic o [UNK] [UNK] r x p l ic o [UNK] [UNK] [SEP]'}, {'score': 0.028625531122088432, 'token': 10237, 'token_str': 'ka', 'sequence': '[CLS] [UNK] [UNK] x p [MASK] ic o [UNK] [UNK] ka x p l ic o [UNK] [UNK] [SEP]'}]\n",
      "Unexpected format for prediction at index 0: [{'score': 0.04890846088528633, 'token': 10237, 'token_str': 'ka', 'sequence': '[CLS] [UNK] ka ke [UNK] [UNK] [UNK] ro ma ka [MASK] [UNK] tanwe [UNK] [UNK] [UNK] [SEP]'}, {'score': 0.03551628068089485, 'token': 11817, 'token_str': 'hon', 'sequence': '[CLS] [UNK] ka ke [UNK] [UNK] [UNK] ro ma hon [MASK] [UNK] tanwe [UNK] [UNK] [UNK] [SEP]'}, {'score': 0.03179621696472168, 'token': 11009, 'token_str': 'ke', 'sequence': '[CLS] [UNK] ka ke [UNK] [UNK] [UNK] ro ma ke [MASK] [UNK] tanwe [UNK] [UNK] [UNK] [SEP]'}, {'score': 0.03145861625671387, 'token': 143, 'token_str': 'a', 'sequence': '[CLS] [UNK] ka ke [UNK] [UNK] [UNK] ro ma a [MASK] [UNK] tanwe [UNK] [UNK] [UNK] [SEP]'}, {'score': 0.03035864420235157, 'token': 10900, 'token_str': 'ki', 'sequence': '[CLS] [UNK] ka ke [UNK] [UNK] [UNK] ro ma ki [MASK] [UNK] tanwe [UNK] [UNK] [UNK] [SEP]'}]\n",
      "Unexpected format for prediction at index 1: [{'score': 0.06391525268554688, 'token': 10116, 'token_str': '##i', 'sequence': '[CLS] [UNK] ka ke [UNK] [UNK] [UNK] ro ma [MASK]i [UNK] tanwe [UNK] [UNK] [UNK] [SEP]'}, {'score': 0.049637168645858765, 'token': 10112, 'token_str': '##a', 'sequence': '[CLS] [UNK] ka ke [UNK] [UNK] [UNK] ro ma [MASK]a [UNK] tanwe [UNK] [UNK] [UNK] [SEP]'}, {'score': 0.03621755167841911, 'token': 10237, 'token_str': 'ka', 'sequence': '[CLS] [UNK] ka ke [UNK] [UNK] [UNK] ro ma [MASK] ka [UNK] tanwe [UNK] [UNK] [UNK] [SEP]'}, {'score': 0.03474107384681702, 'token': 143, 'token_str': 'a', 'sequence': '[CLS] [UNK] ka ke [UNK] [UNK] [UNK] ro ma [MASK] a [UNK] tanwe [UNK] [UNK] [UNK] [SEP]'}, {'score': 0.03271569684147835, 'token': 151, 'token_str': 'i', 'sequence': '[CLS] [UNK] ka ke [UNK] [UNK] [UNK] ro ma [MASK] i [UNK] tanwe [UNK] [UNK] [UNK] [SEP]'}]\n",
      "Unexpected format for prediction at index 0: [{'score': 0.05199144408106804, 'token': 10900, 'token_str': 'ki', 'sequence': '[CLS] [UNK] [UNK] mi mera [UNK] [UNK] [UNK] riki ki [MASK] [UNK] [UNK] ronki [UNK] ki [UNK] i [MASK] [UNK] [UNK] [UNK] [SEP]'}, {'score': 0.046763502061367035, 'token': 151, 'token_str': 'i', 'sequence': '[CLS] [UNK] [UNK] mi mera [UNK] [UNK] [UNK] riki i [MASK] [UNK] [UNK] ronki [UNK] ki [UNK] i [MASK] [UNK] [UNK] [UNK] [SEP]'}, {'score': 0.045360784977674484, 'token': 10237, 'token_str': 'ka', 'sequence': '[CLS] [UNK] [UNK] mi mera [UNK] [UNK] [UNK] riki ka [MASK] [UNK] [UNK] ronki [UNK] ki [UNK] i [MASK] [UNK] [UNK] [UNK] [SEP]'}, {'score': 0.04038948938250542, 'token': 143, 'token_str': 'a', 'sequence': '[CLS] [UNK] [UNK] mi mera [UNK] [UNK] [UNK] riki a [MASK] [UNK] [UNK] ronki [UNK] ki [UNK] i [MASK] [UNK] [UNK] [UNK] [SEP]'}, {'score': 0.0324426032602787, 'token': 11009, 'token_str': 'ke', 'sequence': '[CLS] [UNK] [UNK] mi mera [UNK] [UNK] [UNK] riki ke [MASK] [UNK] [UNK] ronki [UNK] ki [UNK] i [MASK] [UNK] [UNK] [UNK] [SEP]'}]\n",
      "Unexpected format for prediction at index 1: [{'score': 0.08425706624984741, 'token': 10116, 'token_str': '##i', 'sequence': '[CLS] [UNK] [UNK] mi mera [UNK] [UNK] [UNK] riki [MASK]i [UNK] [UNK] ronki [UNK] ki [UNK] i [MASK] [UNK] [UNK] [UNK] [SEP]'}, {'score': 0.050001900643110275, 'token': 10237, 'token_str': 'ka', 'sequence': '[CLS] [UNK] [UNK] mi mera [UNK] [UNK] [UNK] riki [MASK] ka [UNK] [UNK] ronki [UNK] ki [UNK] i [MASK] [UNK] [UNK] [UNK] [SEP]'}, {'score': 0.0479850247502327, 'token': 10900, 'token_str': 'ki', 'sequence': '[CLS] [UNK] [UNK] mi mera [UNK] [UNK] [UNK] riki [MASK] ki [UNK] [UNK] ronki [UNK] ki [UNK] i [MASK] [UNK] [UNK] [UNK] [SEP]'}, {'score': 0.03682437166571617, 'token': 10112, 'token_str': '##a', 'sequence': '[CLS] [UNK] [UNK] mi mera [UNK] [UNK] [UNK] riki [MASK]a [UNK] [UNK] ronki [UNK] ki [UNK] i [MASK] [UNK] [UNK] [UNK] [SEP]'}, {'score': 0.02875898964703083, 'token': 143, 'token_str': 'a', 'sequence': '[CLS] [UNK] [UNK] mi mera [UNK] [UNK] [UNK] riki [MASK] a [UNK] [UNK] ronki [UNK] ki [UNK] i [MASK] [UNK] [UNK] [UNK] [SEP]'}]\n",
      "Unexpected format for prediction at index 2: [{'score': 0.061113279312849045, 'token': 10900, 'token_str': 'ki', 'sequence': '[CLS] [UNK] [UNK] mi mera [UNK] [UNK] [UNK] riki [MASK] [MASK] [UNK] [UNK] ronki [UNK] ki [UNK] i ki [UNK] [UNK] [UNK] [SEP]'}, {'score': 0.052845802158117294, 'token': 10237, 'token_str': 'ka', 'sequence': '[CLS] [UNK] [UNK] mi mera [UNK] [UNK] [UNK] riki [MASK] [MASK] [UNK] [UNK] ronki [UNK] ki [UNK] i ka [UNK] [UNK] [UNK] [SEP]'}, {'score': 0.049264442175626755, 'token': 11742, 'token_str': 'ra', 'sequence': '[CLS] [UNK] [UNK] mi mera [UNK] [UNK] [UNK] riki [MASK] [MASK] [UNK] [UNK] ronki [UNK] ki [UNK] i ra [UNK] [UNK] [UNK] [SEP]'}, {'score': 0.03425854817032814, 'token': 143, 'token_str': 'a', 'sequence': '[CLS] [UNK] [UNK] mi mera [UNK] [UNK] [UNK] riki [MASK] [MASK] [UNK] [UNK] ronki [UNK] ki [UNK] i a [UNK] [UNK] [UNK] [SEP]'}, {'score': 0.032772891223430634, 'token': 19716, 'token_str': 'ri', 'sequence': '[CLS] [UNK] [UNK] mi mera [UNK] [UNK] [UNK] riki [MASK] [MASK] [UNK] [UNK] ronki [UNK] ki [UNK] i ri [UNK] [UNK] [UNK] [SEP]'}]\n",
      "Unexpected format for prediction at index 0: [{'score': 0.05027846619486809, 'token': 10237, 'token_str': 'ka', 'sequence': '[CLS] [UNK] [UNK] sho koa [UNK] ka [UNK] ribi [UNK] chomabi [MASK] [UNK] chomabi [UNK] [SEP]'}, {'score': 0.036898717284202576, 'token': 10900, 'token_str': 'ki', 'sequence': '[CLS] [UNK] [UNK] sho koa [UNK] ki [UNK] ribi [UNK] chomabi [MASK] [UNK] chomabi [UNK] [SEP]'}, {'score': 0.03434092923998833, 'token': 11742, 'token_str': 'ra', 'sequence': '[CLS] [UNK] [UNK] sho koa [UNK] ra [UNK] ribi [UNK] chomabi [MASK] [UNK] chomabi [UNK] [SEP]'}, {'score': 0.02900269627571106, 'token': 11009, 'token_str': 'ke', 'sequence': '[CLS] [UNK] [UNK] sho koa [UNK] ke [UNK] ribi [UNK] chomabi [MASK] [UNK] chomabi [UNK] [SEP]'}, {'score': 0.027713336050510406, 'token': 143, 'token_str': 'a', 'sequence': '[CLS] [UNK] [UNK] sho koa [UNK] a [UNK] ribi [UNK] chomabi [MASK] [UNK] chomabi [UNK] [SEP]'}]\n",
      "Unexpected format for prediction at index 1: [{'score': 0.07143329083919525, 'token': 10116, 'token_str': '##i', 'sequence': '[CLS] [UNK] [UNK] sho koa [UNK] [MASK] [UNK] ribi [UNK] chomabii [UNK] chomabi [UNK] [SEP]'}, {'score': 0.06616641581058502, 'token': 10115, 'token_str': '##n', 'sequence': '[CLS] [UNK] [UNK] sho koa [UNK] [MASK] [UNK] ribi [UNK] chomabin [UNK] chomabi [UNK] [SEP]'}, {'score': 0.064344123005867, 'token': 10112, 'token_str': '##a', 'sequence': '[CLS] [UNK] [UNK] sho koa [UNK] [MASK] [UNK] ribi [UNK] chomabia [UNK] chomabi [UNK] [SEP]'}, {'score': 0.05443398654460907, 'token': 10237, 'token_str': 'ka', 'sequence': '[CLS] [UNK] [UNK] sho koa [UNK] [MASK] [UNK] ribi [UNK] chomabi ka [UNK] chomabi [UNK] [SEP]'}, {'score': 0.05341620370745659, 'token': 151, 'token_str': 'i', 'sequence': '[CLS] [UNK] [UNK] sho koa [UNK] [MASK] [UNK] ribi [UNK] chomabi i [UNK] chomabi [UNK] [SEP]'}]\n",
      "Unexpected format for prediction at index 0: [{'score': 0.13893727958202362, 'token': 10900, 'token_str': 'ki', 'sequence': '[CLS] [UNK] [UNK] wa [UNK] [UNK] ki [UNK] wa [MASK] ki [UNK] hon [UNK] r ka [SEP]'}, {'score': 0.09012774378061295, 'token': 10237, 'token_str': 'ka', 'sequence': '[CLS] [UNK] [UNK] wa [UNK] [UNK] ka [UNK] wa [MASK] ki [UNK] hon [UNK] r ka [SEP]'}, {'score': 0.03819787874817848, 'token': 10863, 'token_str': 'bi', 'sequence': '[CLS] [UNK] [UNK] wa [UNK] [UNK] bi [UNK] wa [MASK] ki [UNK] hon [UNK] r ka [SEP]'}, {'score': 0.030963292345404625, 'token': 11742, 'token_str': 'ra', 'sequence': '[CLS] [UNK] [UNK] wa [UNK] [UNK] ra [UNK] wa [MASK] ki [UNK] hon [UNK] r ka [SEP]'}, {'score': 0.027268271893262863, 'token': 19716, 'token_str': 'ri', 'sequence': '[CLS] [UNK] [UNK] wa [UNK] [UNK] ri [UNK] wa [MASK] ki [UNK] hon [UNK] r ka [SEP]'}]\n",
      "Unexpected format for prediction at index 1: [{'score': 0.07790476083755493, 'token': 10900, 'token_str': 'ki', 'sequence': '[CLS] [UNK] [UNK] wa [UNK] [UNK] [MASK] [UNK] wa ki ki [UNK] hon [UNK] r ka [SEP]'}, {'score': 0.05389094725251198, 'token': 11009, 'token_str': 'ke', 'sequence': '[CLS] [UNK] [UNK] wa [UNK] [UNK] [MASK] [UNK] wa ke ki [UNK] hon [UNK] r ka [SEP]'}, {'score': 0.03138132393360138, 'token': 143, 'token_str': 'a', 'sequence': '[CLS] [UNK] [UNK] wa [UNK] [UNK] [MASK] [UNK] wa a ki [UNK] hon [UNK] r ka [SEP]'}, {'score': 0.030887261033058167, 'token': 10237, 'token_str': 'ka', 'sequence': '[CLS] [UNK] [UNK] wa [UNK] [UNK] [MASK] [UNK] wa ka ki [UNK] hon [UNK] r ka [SEP]'}, {'score': 0.02867213450372219, 'token': 10863, 'token_str': 'bi', 'sequence': '[CLS] [UNK] [UNK] wa [UNK] [UNK] [MASK] [UNK] wa bi ki [UNK] hon [UNK] r ka [SEP]'}]\n",
      "Unexpected format for prediction at index 0: [{'score': 0.07458794862031937, 'token': 151, 'token_str': 'i', 'sequence': '[CLS] [UNK] hoko i [UNK] [UNK] [UNK] ria [UNK] [UNK] ri [UNK] [UNK] bi [UNK] [MASK] [MASK] [UNK] [UNK] [UNK] [UNK] [SEP]'}, {'score': 0.049557164311409, 'token': 11009, 'token_str': 'ke', 'sequence': '[CLS] [UNK] hoko ke [UNK] [UNK] [UNK] ria [UNK] [UNK] ri [UNK] [UNK] bi [UNK] [MASK] [MASK] [UNK] [UNK] [UNK] [UNK] [SEP]'}, {'score': 0.04105863720178604, 'token': 10237, 'token_str': 'ka', 'sequence': '[CLS] [UNK] hoko ka [UNK] [UNK] [UNK] ria [UNK] [UNK] ri [UNK] [UNK] bi [UNK] [MASK] [MASK] [UNK] [UNK] [UNK] [UNK] [SEP]'}, {'score': 0.03907465562224388, 'token': 10900, 'token_str': 'ki', 'sequence': '[CLS] [UNK] hoko ki [UNK] [UNK] [UNK] ria [UNK] [UNK] ri [UNK] [UNK] bi [UNK] [MASK] [MASK] [UNK] [UNK] [UNK] [UNK] [SEP]'}, {'score': 0.03859942778944969, 'token': 143, 'token_str': 'a', 'sequence': '[CLS] [UNK] hoko a [UNK] [UNK] [UNK] ria [UNK] [UNK] ri [UNK] [UNK] bi [UNK] [MASK] [MASK] [UNK] [UNK] [UNK] [UNK] [SEP]'}]\n",
      "Unexpected format for prediction at index 1: [{'score': 0.0558253712952137, 'token': 10863, 'token_str': 'bi', 'sequence': '[CLS] [UNK] hoko [MASK] [UNK] [UNK] [UNK] ria [UNK] [UNK] ri [UNK] [UNK] bi [UNK] bi [MASK] [UNK] [UNK] [UNK] [UNK] [SEP]'}, {'score': 0.03445345163345337, 'token': 10237, 'token_str': 'ka', 'sequence': '[CLS] [UNK] hoko [MASK] [UNK] [UNK] [UNK] ria [UNK] [UNK] ri [UNK] [UNK] bi [UNK] ka [MASK] [UNK] [UNK] [UNK] [UNK] [SEP]'}, {'score': 0.03154691308736801, 'token': 10900, 'token_str': 'ki', 'sequence': '[CLS] [UNK] hoko [MASK] [UNK] [UNK] [UNK] ria [UNK] [UNK] ri [UNK] [UNK] bi [UNK] ki [MASK] [UNK] [UNK] [UNK] [UNK] [SEP]'}, {'score': 0.023205522447824478, 'token': 63135, 'token_str': 'yama', 'sequence': '[CLS] [UNK] hoko [MASK] [UNK] [UNK] [UNK] ria [UNK] [UNK] ri [UNK] [UNK] bi [UNK] yama [MASK] [UNK] [UNK] [UNK] [UNK] [SEP]'}, {'score': 0.023000191897153854, 'token': 19716, 'token_str': 'ri', 'sequence': '[CLS] [UNK] hoko [MASK] [UNK] [UNK] [UNK] ria [UNK] [UNK] ri [UNK] [UNK] bi [UNK] ri [MASK] [UNK] [UNK] [UNK] [UNK] [SEP]'}]\n",
      "Unexpected format for prediction at index 2: [{'score': 0.1029597669839859, 'token': 10116, 'token_str': '##i', 'sequence': '[CLS] [UNK] hoko [MASK] [UNK] [UNK] [UNK] ria [UNK] [UNK] ri [UNK] [UNK] bi [UNK] [MASK]i [UNK] [UNK] [UNK] [UNK] [SEP]'}, {'score': 0.04790389537811279, 'token': 10112, 'token_str': '##a', 'sequence': '[CLS] [UNK] hoko [MASK] [UNK] [UNK] [UNK] ria [UNK] [UNK] ri [UNK] [UNK] bi [UNK] [MASK]a [UNK] [UNK] [UNK] [UNK] [SEP]'}, {'score': 0.035219457000494, 'token': 10457, 'token_str': '##ko', 'sequence': '[CLS] [UNK] hoko [MASK] [UNK] [UNK] [UNK] ria [UNK] [UNK] ri [UNK] [UNK] bi [UNK] [MASK]ko [UNK] [UNK] [UNK] [UNK] [SEP]'}, {'score': 0.031610164791345596, 'token': 10115, 'token_str': '##n', 'sequence': '[CLS] [UNK] hoko [MASK] [UNK] [UNK] [UNK] ria [UNK] [UNK] ri [UNK] [UNK] bi [UNK] [MASK]n [UNK] [UNK] [UNK] [UNK] [SEP]'}, {'score': 0.03085065819323063, 'token': 11009, 'token_str': 'ke', 'sequence': '[CLS] [UNK] hoko [MASK] [UNK] [UNK] [UNK] ria [UNK] [UNK] ri [UNK] [UNK] bi [UNK] [MASK] ke [UNK] [UNK] [UNK] [UNK] [SEP]'}]\n",
      "Unexpected format for prediction at index 0: [{'score': 0.05764740705490112, 'token': 143, 'token_str': 'a', 'sequence': '[CLS] [UNK] [UNK] [UNK] [UNK] [UNK] a [UNK] [UNK] [UNK] h [UNK] biribi [UNK] [UNK] [MASK] [SEP]'}, {'score': 0.05446266010403633, 'token': 10237, 'token_str': 'ka', 'sequence': '[CLS] [UNK] [UNK] [UNK] [UNK] [UNK] ka [UNK] [UNK] [UNK] h [UNK] biribi [UNK] [UNK] [MASK] [SEP]'}, {'score': 0.032581627368927, 'token': 10900, 'token_str': 'ki', 'sequence': '[CLS] [UNK] [UNK] [UNK] [UNK] [UNK] ki [UNK] [UNK] [UNK] h [UNK] biribi [UNK] [UNK] [MASK] [SEP]'}, {'score': 0.02984030172228813, 'token': 11009, 'token_str': 'ke', 'sequence': '[CLS] [UNK] [UNK] [UNK] [UNK] [UNK] ke [UNK] [UNK] [UNK] h [UNK] biribi [UNK] [UNK] [MASK] [SEP]'}, {'score': 0.029140295460820198, 'token': 11742, 'token_str': 'ra', 'sequence': '[CLS] [UNK] [UNK] [UNK] [UNK] [UNK] ra [UNK] [UNK] [UNK] h [UNK] biribi [UNK] [UNK] [MASK] [SEP]'}]\n",
      "Unexpected format for prediction at index 1: [{'score': 0.04513785243034363, 'token': 143, 'token_str': 'a', 'sequence': '[CLS] [UNK] [UNK] [UNK] [UNK] [UNK] [MASK] [UNK] [UNK] [UNK] h [UNK] biribi [UNK] [UNK] a [SEP]'}, {'score': 0.038637880235910416, 'token': 10237, 'token_str': 'ka', 'sequence': '[CLS] [UNK] [UNK] [UNK] [UNK] [UNK] [MASK] [UNK] [UNK] [UNK] h [UNK] biribi [UNK] [UNK] ka [SEP]'}, {'score': 0.028469283133745193, 'token': 11742, 'token_str': 'ra', 'sequence': '[CLS] [UNK] [UNK] [UNK] [UNK] [UNK] [MASK] [UNK] [UNK] [UNK] h [UNK] biribi [UNK] [UNK] ra [SEP]'}, {'score': 0.025814803317189217, 'token': 151, 'token_str': 'i', 'sequence': '[CLS] [UNK] [UNK] [UNK] [UNK] [UNK] [MASK] [UNK] [UNK] [UNK] h [UNK] biribi [UNK] [UNK] i [SEP]'}, {'score': 0.022995661944150925, 'token': 10900, 'token_str': 'ki', 'sequence': '[CLS] [UNK] [UNK] [UNK] [UNK] [UNK] [MASK] [UNK] [UNK] [UNK] h [UNK] biribi [UNK] [UNK] ki [SEP]'}]\n",
      "Unexpected format for prediction at index 0: [{'score': 0.3751455843448639, 'token': 10358, 'token_str': '##ka', 'sequence': '[CLS] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] ka tian [UNK] kinka [UNK] [UNK] i an [MASK] bo [SEP]'}, {'score': 0.36837655305862427, 'token': 10523, 'token_str': '##ki', 'sequence': '[CLS] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] ka tian [UNK] kinki [UNK] [UNK] i an [MASK] bo [SEP]'}, {'score': 0.03500226140022278, 'token': 10457, 'token_str': '##ko', 'sequence': '[CLS] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] ka tian [UNK] kinko [UNK] [UNK] i an [MASK] bo [SEP]'}, {'score': 0.029453333467245102, 'token': 10499, 'token_str': '##ke', 'sequence': '[CLS] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] ka tian [UNK] kinke [UNK] [UNK] i an [MASK] bo [SEP]'}, {'score': 0.01185709610581398, 'token': 10900, 'token_str': 'ki', 'sequence': '[CLS] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] ka tian [UNK] kin ki [UNK] [UNK] i an [MASK] bo [SEP]'}]\n",
      "Unexpected format for prediction at index 1: [{'score': 0.06980115920305252, 'token': 151, 'token_str': 'i', 'sequence': '[CLS] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] ka tian [UNK] kin [MASK] [UNK] [UNK] i an i bo [SEP]'}, {'score': 0.06767980754375458, 'token': 143, 'token_str': 'a', 'sequence': '[CLS] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] ka tian [UNK] kin [MASK] [UNK] [UNK] i an a bo [SEP]'}, {'score': 0.053090427070856094, 'token': 157, 'token_str': 'o', 'sequence': '[CLS] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] ka tian [UNK] kin [MASK] [UNK] [UNK] i an o bo [SEP]'}, {'score': 0.04474565014243126, 'token': 10144, 'token_str': 'an', 'sequence': '[CLS] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] ka tian [UNK] kin [MASK] [UNK] [UNK] i an an bo [SEP]'}, {'score': 0.025838760659098625, 'token': 147, 'token_str': 'e', 'sequence': '[CLS] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] ka tian [UNK] kin [MASK] [UNK] [UNK] i an e bo [SEP]'}]\n",
      "Unexpected format for prediction at index 0: [{'score': 0.0742085874080658, 'token': 143, 'token_str': 'a', 'sequence': '[CLS] a an ka [MASK] ti [UNK] [UNK] [UNK] [UNK] ai [UNK] [UNK] [UNK] [UNK] [SEP]'}, {'score': 0.042592793703079224, 'token': 10144, 'token_str': 'an', 'sequence': '[CLS] an an ka [MASK] ti [UNK] [UNK] [UNK] [UNK] ai [UNK] [UNK] [UNK] [UNK] [SEP]'}, {'score': 0.033834706991910934, 'token': 151, 'token_str': 'i', 'sequence': '[CLS] i an ka [MASK] ti [UNK] [UNK] [UNK] [UNK] ai [UNK] [UNK] [UNK] [UNK] [SEP]'}, {'score': 0.025411520153284073, 'token': 10237, 'token_str': 'ka', 'sequence': '[CLS] ka an ka [MASK] ti [UNK] [UNK] [UNK] [UNK] ai [UNK] [UNK] [UNK] [UNK] [SEP]'}, {'score': 0.021103598177433014, 'token': 10507, 'token_str': 'ma', 'sequence': '[CLS] ma an ka [MASK] ti [UNK] [UNK] [UNK] [UNK] ai [UNK] [UNK] [UNK] [UNK] [SEP]'}]\n",
      "Unexpected format for prediction at index 1: [{'score': 0.040560461580753326, 'token': 10237, 'token_str': 'ka', 'sequence': '[CLS] [MASK] an ka ka ti [UNK] [UNK] [UNK] [UNK] ai [UNK] [UNK] [UNK] [UNK] [SEP]'}, {'score': 0.0401720367372036, 'token': 143, 'token_str': 'a', 'sequence': '[CLS] [MASK] an ka a ti [UNK] [UNK] [UNK] [UNK] ai [UNK] [UNK] [UNK] [UNK] [SEP]'}, {'score': 0.03826288506388664, 'token': 161, 'token_str': 's', 'sequence': '[CLS] [MASK] an ka s ti [UNK] [UNK] [UNK] [UNK] ai [UNK] [UNK] [UNK] [UNK] [SEP]'}, {'score': 0.03116653859615326, 'token': 11009, 'token_str': 'ke', 'sequence': '[CLS] [MASK] an ka ke ti [UNK] [UNK] [UNK] [UNK] ai [UNK] [UNK] [UNK] [UNK] [SEP]'}, {'score': 0.02853686735033989, 'token': 151, 'token_str': 'i', 'sequence': '[CLS] [MASK] an ka i ti [UNK] [UNK] [UNK] [UNK] ai [UNK] [UNK] [UNK] [UNK] [SEP]'}]\n",
      "Unexpected format for prediction at index 0: [{'score': 0.08803217113018036, 'token': 10240, 'token_str': 'ha', 'sequence': '[CLS] [UNK] pe ha ha [UNK] [UNK] [UNK] iki [UNK] pon [UNK] [UNK] ka [UNK] [UNK] pon [MASK] [SEP]'}, {'score': 0.04859887808561325, 'token': 11817, 'token_str': 'hon', 'sequence': '[CLS] [UNK] pe ha hon [UNK] [UNK] [UNK] iki [UNK] pon [UNK] [UNK] ka [UNK] [UNK] pon [MASK] [SEP]'}, {'score': 0.04623500257730484, 'token': 151, 'token_str': 'i', 'sequence': '[CLS] [UNK] pe ha i [UNK] [UNK] [UNK] iki [UNK] pon [UNK] [UNK] ka [UNK] [UNK] pon [MASK] [SEP]'}, {'score': 0.03558412566781044, 'token': 143, 'token_str': 'a', 'sequence': '[CLS] [UNK] pe ha a [UNK] [UNK] [UNK] iki [UNK] pon [UNK] [UNK] ka [UNK] [UNK] pon [MASK] [SEP]'}, {'score': 0.031728990375995636, 'token': 11009, 'token_str': 'ke', 'sequence': '[CLS] [UNK] pe ha ke [UNK] [UNK] [UNK] iki [UNK] pon [UNK] [UNK] ka [UNK] [UNK] pon [MASK] [SEP]'}]\n",
      "Unexpected format for prediction at index 1: [{'score': 0.11884334683418274, 'token': 10116, 'token_str': '##i', 'sequence': '[CLS] [UNK] pe ha [MASK] [UNK] [UNK] [UNK] iki [UNK] pon [UNK] [UNK] ka [UNK] [UNK] poni [SEP]'}, {'score': 0.07017042487859726, 'token': 151, 'token_str': 'i', 'sequence': '[CLS] [UNK] pe ha [MASK] [UNK] [UNK] [UNK] iki [UNK] pon [UNK] [UNK] ka [UNK] [UNK] pon i [SEP]'}, {'score': 0.04665593057870865, 'token': 10523, 'token_str': '##ki', 'sequence': '[CLS] [UNK] pe ha [MASK] [UNK] [UNK] [UNK] iki [UNK] pon [UNK] [UNK] ka [UNK] [UNK] ponki [SEP]'}, {'score': 0.04332844167947769, 'token': 143, 'token_str': 'a', 'sequence': '[CLS] [UNK] pe ha [MASK] [UNK] [UNK] [UNK] iki [UNK] pon [UNK] [UNK] ka [UNK] [UNK] pon a [SEP]'}, {'score': 0.04203012213110924, 'token': 10900, 'token_str': 'ki', 'sequence': '[CLS] [UNK] pe ha [MASK] [UNK] [UNK] [UNK] iki [UNK] pon [UNK] [UNK] ka [UNK] [UNK] pon ki [SEP]'}]\n",
      "Unexpected format for prediction at index 0: [{'score': 0.26160579919815063, 'token': 10116, 'token_str': '##i', 'sequence': '[CLS] [UNK] [UNK] [UNK] [UNK] yai [UNK] we [UNK] [UNK] biribi [MASK] [UNK] we [UNK] [UNK] [UNK] biribi [SEP]'}, {'score': 0.050807300955057144, 'token': 11009, 'token_str': 'ke', 'sequence': '[CLS] [UNK] [UNK] [UNK] [UNK] ya ke [UNK] we [UNK] [UNK] biribi [MASK] [UNK] we [UNK] [UNK] [UNK] biribi [SEP]'}, {'score': 0.046021897345781326, 'token': 10112, 'token_str': '##a', 'sequence': '[CLS] [UNK] [UNK] [UNK] [UNK] yaa [UNK] we [UNK] [UNK] biribi [MASK] [UNK] we [UNK] [UNK] [UNK] biribi [SEP]'}, {'score': 0.03563385456800461, 'token': 10237, 'token_str': 'ka', 'sequence': '[CLS] [UNK] [UNK] [UNK] [UNK] ya ka [UNK] we [UNK] [UNK] biribi [MASK] [UNK] we [UNK] [UNK] [UNK] biribi [SEP]'}, {'score': 0.03533430024981499, 'token': 10863, 'token_str': 'bi', 'sequence': '[CLS] [UNK] [UNK] [UNK] [UNK] ya bi [UNK] we [UNK] [UNK] biribi [MASK] [UNK] we [UNK] [UNK] [UNK] biribi [SEP]'}]\n",
      "Unexpected format for prediction at index 1: [{'score': 0.06909732520580292, 'token': 10237, 'token_str': 'ka', 'sequence': '[CLS] [UNK] [UNK] [UNK] [UNK] ya [MASK] [UNK] we [UNK] [UNK] biribi ka [UNK] we [UNK] [UNK] [UNK] biribi [SEP]'}, {'score': 0.062418486922979355, 'token': 151, 'token_str': 'i', 'sequence': '[CLS] [UNK] [UNK] [UNK] [UNK] ya [MASK] [UNK] we [UNK] [UNK] biribi i [UNK] we [UNK] [UNK] [UNK] biribi [SEP]'}, {'score': 0.054279688745737076, 'token': 143, 'token_str': 'a', 'sequence': '[CLS] [UNK] [UNK] [UNK] [UNK] ya [MASK] [UNK] we [UNK] [UNK] biribi a [UNK] we [UNK] [UNK] [UNK] biribi [SEP]'}, {'score': 0.04889008030295372, 'token': 11312, 'token_str': 'we', 'sequence': '[CLS] [UNK] [UNK] [UNK] [UNK] ya [MASK] [UNK] we [UNK] [UNK] biribi we [UNK] we [UNK] [UNK] [UNK] biribi [SEP]'}, {'score': 0.04448222368955612, 'token': 10900, 'token_str': 'ki', 'sequence': '[CLS] [UNK] [UNK] [UNK] [UNK] ya [MASK] [UNK] we [UNK] [UNK] biribi ki [UNK] we [UNK] [UNK] [UNK] biribi [SEP]'}]\n",
      "Unexpected format for prediction at index 0: [{'score': 0.088142029941082, 'token': 16999, 'token_str': 'pi', 'sequence': '[CLS] [UNK] pi pi ama [MASK] [UNK] [UNK] yama [UNK] ita hako [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [MASK] bo [UNK] [SEP]'}, {'score': 0.07265873998403549, 'token': 143, 'token_str': 'a', 'sequence': '[CLS] [UNK] a pi ama [MASK] [UNK] [UNK] yama [UNK] ita hako [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [MASK] bo [UNK] [SEP]'}, {'score': 0.03499069809913635, 'token': 10507, 'token_str': 'ma', 'sequence': '[CLS] [UNK] ma pi ama [MASK] [UNK] [UNK] yama [UNK] ita hako [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [MASK] bo [UNK] [SEP]'}, {'score': 0.03394511342048645, 'token': 151, 'token_str': 'i', 'sequence': '[CLS] [UNK] i pi ama [MASK] [UNK] [UNK] yama [UNK] ita hako [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [MASK] bo [UNK] [SEP]'}, {'score': 0.022092550992965698, 'token': 19716, 'token_str': 'ri', 'sequence': '[CLS] [UNK] ri pi ama [MASK] [UNK] [UNK] yama [UNK] ita hako [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [MASK] bo [UNK] [SEP]'}]\n",
      "Unexpected format for prediction at index 1: [{'score': 0.981391191482544, 'token': 11117, 'token_str': 'bo', 'sequence': '[CLS] [UNK] [MASK] pi ama bo [UNK] [UNK] yama [UNK] ita hako [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [MASK] bo [UNK] [SEP]'}, {'score': 0.0013396249851211905, 'token': 10154, 'token_str': 'do', 'sequence': '[CLS] [UNK] [MASK] pi ama do [UNK] [UNK] yama [UNK] ita hako [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [MASK] bo [UNK] [SEP]'}, {'score': 0.001127829891629517, 'token': 151, 'token_str': 'i', 'sequence': '[CLS] [UNK] [MASK] pi ama i [UNK] [UNK] yama [UNK] ita hako [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [MASK] bo [UNK] [SEP]'}, {'score': 0.0009391056373715401, 'token': 143, 'token_str': 'a', 'sequence': '[CLS] [UNK] [MASK] pi ama a [UNK] [UNK] yama [UNK] ita hako [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [MASK] bo [UNK] [SEP]'}, {'score': 0.0008745958912186325, 'token': 10202, 'token_str': 'pa', 'sequence': '[CLS] [UNK] [MASK] pi ama pa [UNK] [UNK] yama [UNK] ita hako [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [MASK] bo [UNK] [SEP]'}]\n",
      "Unexpected format for prediction at index 2: [{'score': 0.1696675568819046, 'token': 22079, 'token_str': 'ama', 'sequence': '[CLS] [UNK] [MASK] pi ama [MASK] [UNK] [UNK] yama [UNK] ita hako [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] ama bo [UNK] [SEP]'}, {'score': 0.07388991862535477, 'token': 19716, 'token_str': 'ri', 'sequence': '[CLS] [UNK] [MASK] pi ama [MASK] [UNK] [UNK] yama [UNK] ita hako [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] ri bo [UNK] [SEP]'}, {'score': 0.04050687700510025, 'token': 11117, 'token_str': 'bo', 'sequence': '[CLS] [UNK] [MASK] pi ama [MASK] [UNK] [UNK] yama [UNK] ita hako [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] bo bo [UNK] [SEP]'}, {'score': 0.03094383142888546, 'token': 10507, 'token_str': 'ma', 'sequence': '[CLS] [UNK] [MASK] pi ama [MASK] [UNK] [UNK] yama [UNK] ita hako [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] ma bo [UNK] [SEP]'}, {'score': 0.026966288685798645, 'token': 16999, 'token_str': 'pi', 'sequence': '[CLS] [UNK] [MASK] pi ama [MASK] [UNK] [UNK] yama [UNK] ita hako [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] pi bo [UNK] [SEP]'}]\n",
      "Unexpected format for prediction at index 0: [{'score': 0.5846513509750366, 'token': 10116, 'token_str': '##i', 'sequence': '[CLS] [UNK] [UNK] tian [UNK] a [UNK] ki [UNK] ro kon ki [UNK] ki [UNK] peni [UNK] [UNK] [MASK] [UNK] [UNK] [MASK] [SEP]'}, {'score': 0.03811364248394966, 'token': 10111, 'token_str': '##e', 'sequence': '[CLS] [UNK] [UNK] tian [UNK] a [UNK] ki [UNK] ro kon ki [UNK] ki [UNK] pene [UNK] [UNK] [MASK] [UNK] [UNK] [MASK] [SEP]'}, {'score': 0.02686639130115509, 'token': 10132, 'token_str': '##o', 'sequence': '[CLS] [UNK] [UNK] tian [UNK] a [UNK] ki [UNK] ro kon ki [UNK] ki [UNK] peno [UNK] [UNK] [MASK] [UNK] [UNK] [MASK] [SEP]'}, {'score': 0.024593058973550797, 'token': 10273, 'token_str': '##on', 'sequence': '[CLS] [UNK] [UNK] tian [UNK] a [UNK] ki [UNK] ro kon ki [UNK] ki [UNK] penon [UNK] [UNK] [MASK] [UNK] [UNK] [MASK] [SEP]'}, {'score': 0.02405717968940735, 'token': 10112, 'token_str': '##a', 'sequence': '[CLS] [UNK] [UNK] tian [UNK] a [UNK] ki [UNK] ro kon ki [UNK] ki [UNK] pena [UNK] [UNK] [MASK] [UNK] [UNK] [MASK] [SEP]'}]\n",
      "Unexpected format for prediction at index 1: [{'score': 0.08926374465227127, 'token': 10900, 'token_str': 'ki', 'sequence': '[CLS] [UNK] [UNK] tian [UNK] a [UNK] ki [UNK] ro kon ki [UNK] ki [UNK] pen [MASK] [UNK] [UNK] ki [UNK] [UNK] [MASK] [SEP]'}, {'score': 0.04691534861922264, 'token': 10237, 'token_str': 'ka', 'sequence': '[CLS] [UNK] [UNK] tian [UNK] a [UNK] ki [UNK] ro kon ki [UNK] ki [UNK] pen [MASK] [UNK] [UNK] ka [UNK] [UNK] [MASK] [SEP]'}, {'score': 0.037063948810100555, 'token': 73845, 'token_str': 'tian', 'sequence': '[CLS] [UNK] [UNK] tian [UNK] a [UNK] ki [UNK] ro kon ki [UNK] ki [UNK] pen [MASK] [UNK] [UNK] tian [UNK] [UNK] [MASK] [SEP]'}, {'score': 0.030748125165700912, 'token': 11009, 'token_str': 'ke', 'sequence': '[CLS] [UNK] [UNK] tian [UNK] a [UNK] ki [UNK] ro kon ki [UNK] ki [UNK] pen [MASK] [UNK] [UNK] ke [UNK] [UNK] [MASK] [SEP]'}, {'score': 0.028574764728546143, 'token': 143, 'token_str': 'a', 'sequence': '[CLS] [UNK] [UNK] tian [UNK] a [UNK] ki [UNK] ro kon ki [UNK] ki [UNK] pen [MASK] [UNK] [UNK] a [UNK] [UNK] [MASK] [SEP]'}]\n",
      "Unexpected format for prediction at index 2: [{'score': 0.06418328732252121, 'token': 10900, 'token_str': 'ki', 'sequence': '[CLS] [UNK] [UNK] tian [UNK] a [UNK] ki [UNK] ro kon ki [UNK] ki [UNK] pen [MASK] [UNK] [UNK] [MASK] [UNK] [UNK] ki [SEP]'}, {'score': 0.049435265362262726, 'token': 10237, 'token_str': 'ka', 'sequence': '[CLS] [UNK] [UNK] tian [UNK] a [UNK] ki [UNK] ro kon ki [UNK] ki [UNK] pen [MASK] [UNK] [UNK] [MASK] [UNK] [UNK] ka [SEP]'}, {'score': 0.03733814135193825, 'token': 73845, 'token_str': 'tian', 'sequence': '[CLS] [UNK] [UNK] tian [UNK] a [UNK] ki [UNK] ro kon ki [UNK] ki [UNK] pen [MASK] [UNK] [UNK] [MASK] [UNK] [UNK] tian [SEP]'}, {'score': 0.031599849462509155, 'token': 143, 'token_str': 'a', 'sequence': '[CLS] [UNK] [UNK] tian [UNK] a [UNK] ki [UNK] ro kon ki [UNK] ki [UNK] pen [MASK] [UNK] [UNK] [MASK] [UNK] [UNK] a [SEP]'}, {'score': 0.027539033442735672, 'token': 11279, 'token_str': 'ai', 'sequence': '[CLS] [UNK] [UNK] tian [UNK] a [UNK] ki [UNK] ro kon ki [UNK] ki [UNK] pen [MASK] [UNK] [UNK] [MASK] [UNK] [UNK] ai [SEP]'}]\n",
      "Unexpected format for prediction at index 0: [{'score': 0.05040290579199791, 'token': 10237, 'token_str': 'ka', 'sequence': '[CLS] [UNK] [UNK] ka [UNK] ka tian [UNK] [UNK] konin [UNK] i v e res [UNK] ketian [MASK] [SEP]'}, {'score': 0.0384567528963089, 'token': 143, 'token_str': 'a', 'sequence': '[CLS] [UNK] [UNK] a [UNK] ka tian [UNK] [UNK] konin [UNK] i v e res [UNK] ketian [MASK] [SEP]'}, {'score': 0.03218044340610504, 'token': 11279, 'token_str': 'ai', 'sequence': '[CLS] [UNK] [UNK] ai [UNK] ka tian [UNK] [UNK] konin [UNK] i v e res [UNK] ketian [MASK] [SEP]'}, {'score': 0.029170701280236244, 'token': 73845, 'token_str': 'tian', 'sequence': '[CLS] [UNK] [UNK] tian [UNK] ka tian [UNK] [UNK] konin [UNK] i v e res [UNK] ketian [MASK] [SEP]'}, {'score': 0.028959166258573532, 'token': 151, 'token_str': 'i', 'sequence': '[CLS] [UNK] [UNK] i [UNK] ka tian [UNK] [UNK] konin [UNK] i v e res [UNK] ketian [MASK] [SEP]'}]\n",
      "Unexpected format for prediction at index 1: [{'score': 0.0999058187007904, 'token': 151, 'token_str': 'i', 'sequence': '[CLS] [UNK] [UNK] [MASK] [UNK] ka tian [UNK] [UNK] konin [UNK] i v e res [UNK] ketian i [SEP]'}, {'score': 0.043768540024757385, 'token': 143, 'token_str': 'a', 'sequence': '[CLS] [UNK] [UNK] [MASK] [UNK] ka tian [UNK] [UNK] konin [UNK] i v e res [UNK] ketian a [SEP]'}, {'score': 0.04275044426321983, 'token': 10115, 'token_str': '##n', 'sequence': '[CLS] [UNK] [UNK] [MASK] [UNK] ka tian [UNK] [UNK] konin [UNK] i v e res [UNK] ketiann [SEP]'}, {'score': 0.03231425955891609, 'token': 12351, 'token_str': '##we', 'sequence': '[CLS] [UNK] [UNK] [MASK] [UNK] ka tian [UNK] [UNK] konin [UNK] i v e res [UNK] ketianwe [SEP]'}, {'score': 0.029289109632372856, 'token': 10116, 'token_str': '##i', 'sequence': '[CLS] [UNK] [UNK] [MASK] [UNK] ka tian [UNK] [UNK] konin [UNK] i v e res [UNK] ketiani [SEP]'}]\n",
      "Unexpected format for prediction at index 0: [{'score': 0.057457856833934784, 'token': 19716, 'token_str': 'ri', 'sequence': '[CLS] [UNK] [UNK] ai [UNK] [UNK] ri mpan ai [UNK] [MASK] [UNK] ran ai [UNK] [UNK] [SEP]'}, {'score': 0.036718882620334625, 'token': 11742, 'token_str': 'ra', 'sequence': '[CLS] [UNK] [UNK] ai [UNK] [UNK] ra mpan ai [UNK] [MASK] [UNK] ran ai [UNK] [UNK] [SEP]'}, {'score': 0.034524839371442795, 'token': 143, 'token_str': 'a', 'sequence': '[CLS] [UNK] [UNK] ai [UNK] [UNK] a mpan ai [UNK] [MASK] [UNK] ran ai [UNK] [UNK] [SEP]'}, {'score': 0.030490241944789886, 'token': 160, 'token_str': 'r', 'sequence': '[CLS] [UNK] [UNK] ai [UNK] [UNK] r mpan ai [UNK] [MASK] [UNK] ran ai [UNK] [UNK] [SEP]'}, {'score': 0.024973316118121147, 'token': 151, 'token_str': 'i', 'sequence': '[CLS] [UNK] [UNK] ai [UNK] [UNK] i mpan ai [UNK] [MASK] [UNK] ran ai [UNK] [UNK] [SEP]'}]\n",
      "Unexpected format for prediction at index 1: [{'score': 0.13308627903461456, 'token': 11279, 'token_str': 'ai', 'sequence': '[CLS] [UNK] [UNK] ai [UNK] [UNK] [MASK] mpan ai [UNK] ai [UNK] ran ai [UNK] [UNK] [SEP]'}, {'score': 0.039284463971853256, 'token': 10237, 'token_str': 'ka', 'sequence': '[CLS] [UNK] [UNK] ai [UNK] [UNK] [MASK] mpan ai [UNK] ka [UNK] ran ai [UNK] [UNK] [SEP]'}, {'score': 0.035617176443338394, 'token': 10863, 'token_str': 'bi', 'sequence': '[CLS] [UNK] [UNK] ai [UNK] [UNK] [MASK] mpan ai [UNK] bi [UNK] ran ai [UNK] [UNK] [SEP]'}, {'score': 0.03387761488556862, 'token': 143, 'token_str': 'a', 'sequence': '[CLS] [UNK] [UNK] ai [UNK] [UNK] [MASK] mpan ai [UNK] a [UNK] ran ai [UNK] [UNK] [SEP]'}, {'score': 0.03286557272076607, 'token': 151, 'token_str': 'i', 'sequence': '[CLS] [UNK] [UNK] ai [UNK] [UNK] [MASK] mpan ai [UNK] i [UNK] ran ai [UNK] [UNK] [SEP]'}]\n",
      "Unexpected format for prediction at index 0: [{'score': 0.06881742924451828, 'token': 10237, 'token_str': 'ka', 'sequence': '[CLS] ka [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [MASK] anai [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] k o [UNK] [SEP]'}, {'score': 0.03902840614318848, 'token': 10900, 'token_str': 'ki', 'sequence': '[CLS] ki [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [MASK] anai [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] k o [UNK] [SEP]'}, {'score': 0.03833676129579544, 'token': 143, 'token_str': 'a', 'sequence': '[CLS] a [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [MASK] anai [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] k o [UNK] [SEP]'}, {'score': 0.03581909090280533, 'token': 10507, 'token_str': 'ma', 'sequence': '[CLS] ma [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [MASK] anai [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] k o [UNK] [SEP]'}, {'score': 0.03212698549032211, 'token': 11009, 'token_str': 'ke', 'sequence': '[CLS] ke [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [MASK] anai [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] k o [UNK] [SEP]'}]\n",
      "Unexpected format for prediction at index 1: [{'score': 0.05617477744817734, 'token': 10237, 'token_str': 'ka', 'sequence': '[CLS] [MASK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] ka anai [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] k o [UNK] [SEP]'}, {'score': 0.04176456481218338, 'token': 143, 'token_str': 'a', 'sequence': '[CLS] [MASK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] a anai [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] k o [UNK] [SEP]'}, {'score': 0.03686064854264259, 'token': 11009, 'token_str': 'ke', 'sequence': '[CLS] [MASK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] ke anai [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] k o [UNK] [SEP]'}, {'score': 0.028176985681056976, 'token': 151, 'token_str': 'i', 'sequence': '[CLS] [MASK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] i anai [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] k o [UNK] [SEP]'}, {'score': 0.026250390335917473, 'token': 10507, 'token_str': 'ma', 'sequence': '[CLS] [MASK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] ma anai [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] k o [UNK] [SEP]'}]\n",
      "Unexpected format for prediction at index 0: [{'score': 0.052406731992959976, 'token': 10237, 'token_str': 'ka', 'sequence': '[CLS] [UNK] ka [UNK] u l c e [MASK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] shama [SEP]'}, {'score': 0.048707589507102966, 'token': 143, 'token_str': 'a', 'sequence': '[CLS] [UNK] a [UNK] u l c e [MASK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] shama [SEP]'}, {'score': 0.03442014753818512, 'token': 10863, 'token_str': 'bi', 'sequence': '[CLS] [UNK] bi [UNK] u l c e [MASK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] shama [SEP]'}, {'score': 0.03189323469996452, 'token': 10900, 'token_str': 'ki', 'sequence': '[CLS] [UNK] ki [UNK] u l c e [MASK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] shama [SEP]'}, {'score': 0.027404213324189186, 'token': 11009, 'token_str': 'ke', 'sequence': '[CLS] [UNK] ke [UNK] u l c e [MASK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] shama [SEP]'}]\n",
      "Unexpected format for prediction at index 1: [{'score': 0.07178516685962677, 'token': 143, 'token_str': 'a', 'sequence': '[CLS] [UNK] [MASK] [UNK] u l c e a [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] shama [SEP]'}, {'score': 0.04396441578865051, 'token': 151, 'token_str': 'i', 'sequence': '[CLS] [UNK] [MASK] [UNK] u l c e i [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] shama [SEP]'}, {'score': 0.02887585200369358, 'token': 10237, 'token_str': 'ka', 'sequence': '[CLS] [UNK] [MASK] [UNK] u l c e ka [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] shama [SEP]'}, {'score': 0.02585187740623951, 'token': 11279, 'token_str': 'ai', 'sequence': '[CLS] [UNK] [MASK] [UNK] u l c e ai [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] shama [SEP]'}, {'score': 0.0245562382042408, 'token': 11009, 'token_str': 'ke', 'sequence': '[CLS] [UNK] [MASK] [UNK] u l c e ke [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] shama [SEP]'}]\n",
      "Unexpected format for prediction at index 0: [{'score': 0.13145698606967926, 'token': 15695, 'token_str': 'ran', 'sequence': '[CLS] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] ran ika [UNK] [UNK] ran ran i [UNK] [MASK] [SEP]'}, {'score': 0.04166515916585922, 'token': 143, 'token_str': 'a', 'sequence': '[CLS] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] ran ika [UNK] [UNK] a ran i [UNK] [MASK] [SEP]'}, {'score': 0.039975766092538834, 'token': 160, 'token_str': 'r', 'sequence': '[CLS] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] ran ika [UNK] [UNK] r ran i [UNK] [MASK] [SEP]'}, {'score': 0.032885972410440445, 'token': 11742, 'token_str': 'ra', 'sequence': '[CLS] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] ran ika [UNK] [UNK] ra ran i [UNK] [MASK] [SEP]'}, {'score': 0.029860906302928925, 'token': 151, 'token_str': 'i', 'sequence': '[CLS] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] ran ika [UNK] [UNK] i ran i [UNK] [MASK] [SEP]'}]\n",
      "Unexpected format for prediction at index 1: [{'score': 0.043639086186885834, 'token': 10237, 'token_str': 'ka', 'sequence': '[CLS] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] ran ika [UNK] [UNK] [MASK] ran i [UNK] ka [SEP]'}, {'score': 0.038687776774168015, 'token': 143, 'token_str': 'a', 'sequence': '[CLS] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] ran ika [UNK] [UNK] [MASK] ran i [UNK] a [SEP]'}, {'score': 0.03278162330389023, 'token': 11009, 'token_str': 'ke', 'sequence': '[CLS] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] ran ika [UNK] [UNK] [MASK] ran i [UNK] ke [SEP]'}, {'score': 0.02830953523516655, 'token': 151, 'token_str': 'i', 'sequence': '[CLS] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] ran ika [UNK] [UNK] [MASK] ran i [UNK] i [SEP]'}, {'score': 0.028057128190994263, 'token': 11742, 'token_str': 'ra', 'sequence': '[CLS] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] ran ika [UNK] [UNK] [MASK] ran i [UNK] ra [SEP]'}]\n",
      "Unexpected format for prediction at index 0: [{'score': 0.24308864772319794, 'token': 10900, 'token_str': 'ki', 'sequence': '[CLS] [UNK] ki [UNK] [UNK] [UNK] [UNK] banon [UNK] [UNK] [MASK] [UNK] [UNK] [UNK] ki [UNK] [SEP]'}, {'score': 0.07403814792633057, 'token': 10237, 'token_str': 'ka', 'sequence': '[CLS] [UNK] ka [UNK] [UNK] [UNK] [UNK] banon [UNK] [UNK] [MASK] [UNK] [UNK] [UNK] ki [UNK] [SEP]'}, {'score': 0.03910215571522713, 'token': 10863, 'token_str': 'bi', 'sequence': '[CLS] [UNK] bi [UNK] [UNK] [UNK] [UNK] banon [UNK] [UNK] [MASK] [UNK] [UNK] [UNK] ki [UNK] [SEP]'}, {'score': 0.030510347336530685, 'token': 11117, 'token_str': 'bo', 'sequence': '[CLS] [UNK] bo [UNK] [UNK] [UNK] [UNK] banon [UNK] [UNK] [MASK] [UNK] [UNK] [UNK] ki [UNK] [SEP]'}, {'score': 0.025776639580726624, 'token': 11009, 'token_str': 'ke', 'sequence': '[CLS] [UNK] ke [UNK] [UNK] [UNK] [UNK] banon [UNK] [UNK] [MASK] [UNK] [UNK] [UNK] ki [UNK] [SEP]'}]\n",
      "Unexpected format for prediction at index 1: [{'score': 0.22662615776062012, 'token': 10900, 'token_str': 'ki', 'sequence': '[CLS] [UNK] [MASK] [UNK] [UNK] [UNK] [UNK] banon [UNK] [UNK] ki [UNK] [UNK] [UNK] ki [UNK] [SEP]'}, {'score': 0.04275714233517647, 'token': 10863, 'token_str': 'bi', 'sequence': '[CLS] [UNK] [MASK] [UNK] [UNK] [UNK] [UNK] banon [UNK] [UNK] bi [UNK] [UNK] [UNK] ki [UNK] [SEP]'}, {'score': 0.04121282696723938, 'token': 10237, 'token_str': 'ka', 'sequence': '[CLS] [UNK] [MASK] [UNK] [UNK] [UNK] [UNK] banon [UNK] [UNK] ka [UNK] [UNK] [UNK] ki [UNK] [SEP]'}, {'score': 0.038454845547676086, 'token': 11117, 'token_str': 'bo', 'sequence': '[CLS] [UNK] [MASK] [UNK] [UNK] [UNK] [UNK] banon [UNK] [UNK] bo [UNK] [UNK] [UNK] ki [UNK] [SEP]'}, {'score': 0.028476404026150703, 'token': 143, 'token_str': 'a', 'sequence': '[CLS] [UNK] [MASK] [UNK] [UNK] [UNK] [UNK] banon [UNK] [UNK] a [UNK] [UNK] [UNK] ki [UNK] [SEP]'}]\n",
      "Accuracy on masked tokens: 2.00%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "\n",
    "mask_token = mbert_tokenizerBPE.mask_token\n",
    "if mask_token is None:\n",
    "    raise ValueError(\"Tokenizer does not have a mask token.\")\n",
    "\n",
    "# Initialize masked language modeling pipeline\n",
    "fill_mask = pipeline(\"fill-mask\", model=modelBPE, tokenizer=mbert_tokenizerBPE)\n",
    "\n",
    "def evaluate_model(tokenized_sentences):\n",
    "    total, correct = 0, 0\n",
    "\n",
    "    for sentence_tokens in tokenized_sentences:\n",
    "        # Ensure there are enough tokens to mask (at least 1 token)\n",
    "        if len(sentence_tokens) < 2:\n",
    "            continue  # Skip very short sentences\n",
    "\n",
    "        # Mask a percentage (15%) of tokens in the sentence\n",
    "        # Ensure at least 1 token is masked\n",
    "        mask_count = max(1, int(0.15 * len(sentence_tokens)))\n",
    "        masked_idx = np.random.choice(len(sentence_tokens), size=mask_count, replace=False)\n",
    "        \n",
    "        # Create a copy of the tokens to preserve the original tokens\n",
    "        original_tokens = sentence_tokens.copy()\n",
    "        \n",
    "        # Mask the selected tokens\n",
    "        for idx in masked_idx:\n",
    "            original_tokens[idx] = mask_token  # Insert the mask token\n",
    "\n",
    "        # print(f\"original sentence: {original_tokens}\")  # Debugging line\n",
    "\n",
    "        # Convert back to string (reconstructed sentence)\n",
    "        masked_sentence = mbert_tokenizerBPE.convert_tokens_to_string(original_tokens)\n",
    "\n",
    "        # Ensure the mask token is included\n",
    "        # print(f\"Masked sentence: {masked_sentence}\")  # Debugging line\n",
    "\n",
    "        # Get model predictions for the masked tokens\n",
    "        try:\n",
    "            predictions = fill_mask(masked_sentence)\n",
    "        except Exception as e:\n",
    "            print(f\"Error with masked sentence: {masked_sentence}\")\n",
    "            print(e)\n",
    "            continue\n",
    "\n",
    "        # Check if predictions match the original tokens\n",
    "        for i, idx in enumerate(masked_idx):\n",
    "                    # Handle unexpected cases where predictions might be empty or misformatted\n",
    "                    if len(predictions) > i and isinstance(predictions[i], dict):\n",
    "                        predicted_token = predictions[i].get(\"token_str\", \"\")\n",
    "                        original_token = sentence_tokens[idx]\n",
    "                        if predicted_token == original_token:\n",
    "                            correct += 1\n",
    "                        total += 1\n",
    "                    else:\n",
    "                        print(f\"Unexpected format for prediction at index {i}: {predictions[i]}\")\n",
    "\n",
    "    accuracy = correct / total if total > 0 else 0\n",
    "    print(f\"Accuracy on masked tokens: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Sample test data (tokenized sentences)\n",
    "evaluate_model(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the mask token is defined\n",
    "mask_token = mbert_tokenizerBPE.mask_token\n",
    "if mask_token is None:\n",
    "    raise ValueError(\"Tokenizer does not have a mask token.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[MASK]'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_token"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langmodels",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
